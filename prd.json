{
  "project_meta": {
    "name": "AnyDocsMCP",
    "version": "v3-quality-score-upgrade",
    "ralph_type": "opencode",
    "opencode_session_id": "deep-init-v3-2026-02-07"
  },
  "backlog": [
    {
      "group": "URL-Discovery-Quality",
      "feature": "URL Discovery accuracy test for all 10 reference doc-sets",
      "description": "Create a test that runs URLDiscovery.discover_urls() against each of the 10 reference doc-sets (live, one-time capture) and records: mode used (sitemap/nav/crawl), number of URLs found, scope detected, version detected. Store results in tests/fixtures/real-world/discovery-baseline.json. Compare against known expected page counts (react ~80, fastapi ~130, tailwind ~180, kubernetes ~60, django ~60, etc.) to verify completeness. Identify sites where discovery misses >20% of known pages.",
      "acceptance_criteria": [
        "Test calls discover_urls() for all 10 start URLs from capture-manifest.json — verified by pytest parametrize",
        "Results stored in discovery-baseline.json with mode, url_count, scope, version per site — verified by JSON schema check",
        "Each site finds at least 50% of its known page count from AppData — verified by comparing against AppData file counts",
        "Discovery mode is logged (sitemap/nav/crawl) for each site — verified by checking mode field in baseline",
        "Sites with <50% coverage are flagged in a warnings array — verified by checking warnings field"
      ],
      "passes": false
    },
    {
      "group": "URL-Discovery-Quality",
      "feature": "Language and locale scope filter for URL discovery",
      "description": "Fix the Django Greek content problem: URLDiscovery currently has no language/locale filtering. When Django's sitemap returns URLs for /el/ (Greek), /ja/ (Japanese), etc., the scraper should filter to the target language (default: /en/). Add locale_filter parameter to discover_urls() that accepts a language code (e.g., 'en') and filters URLs to only include that locale. Auto-detect locale from start_url path (e.g., /en/stable/ → 'en'). Add test with Django sitemap fixture.",
      "acceptance_criteria": [
        "discover_urls() accepts optional locale_filter='en' parameter — verified by unit test",
        "URLs containing /el/, /ja/, /fr/, /zh/ etc. are excluded when locale_filter='en' — verified by test with mock sitemap containing multi-locale URLs",
        "Auto-detection: start_url /en/stable/ auto-sets locale_filter='en' — verified by unit test",
        "Django specifically returns English docs, not Greek — verified by re-scraping Django with locale filter and checking content language",
        "Existing behavior unchanged when locale_filter=None — verified by regression test"
      ],
      "passes": true
    },
    {
      "group": "URL-Discovery-Quality",
      "feature": "Scope detection accuracy test with ground-truth URLs",
      "description": "Create ground-truth URL lists for 5 reference doc-sets (react, fastapi, kubernetes, django, tailwind) by extracting all URLs from their actual scrapes in AppData. Build a test that compares _determine_scope() and _url_in_scope() output against these ground-truth sets. Measure precision (no irrelevant URLs) and recall (no missed URLs). Target: >90% recall, >95% precision.",
      "acceptance_criteria": [
        "Ground-truth URL lists created from AppData source URLs for 5 doc-sets — verified by file existence with >20 URLs each",
        "Test measures precision: ratio of discovered URLs that are in ground-truth — verified by precision >= 0.95",
        "Test measures recall: ratio of ground-truth URLs that were discovered — verified by recall >= 0.90",
        "_determine_scope() returns correct scope for each site — verified by comparing against known scopes",
        "Results stored in tests/fixtures/real-world/scope-accuracy.json — verified by JSON output"
      ],
      "passes": false
    },
    {
      "group": "URL-Discovery-Quality",
      "feature": "Sitemap parser robustness for edge cases",
      "description": "Test and fix SitemapParser for real-world edge cases found across the 10 reference sites: nested sitemap index files (kubernetes), sitemap.xml.gz compressed sitemaps, sitemaps with lastmod/priority filtering, sitemaps behind /robots.txt discovery, and sites without sitemap (should gracefully fall back). Create fixture sitemaps for each edge case.",
      "acceptance_criteria": [
        "Handles sitemap index files (sitemapindex → multiple sitemap URLs) — verified by test with kubernetes-style sitemap index fixture",
        "Handles gzipped sitemaps (.xml.gz) — verified by test with compressed fixture",
        "Gracefully returns empty list for sites without sitemap — verified by test with 404 sitemap response",
        "Filters by lastmod date when refresh_after is set — verified by test with dated sitemap entries",
        "Discovers sitemap URL from robots.txt Sitemap: directive — verified by test with robots.txt fixture"
      ],
      "passes": false
    },
    {
      "group": "URL-Discovery-Quality",
      "feature": "Navigation extraction improvement for SPA documentation sites",
      "description": "React.dev, Tailwind, and Synthflow use JavaScript-rendered navigation that BeautifulSoup cannot parse from raw HTML. Improve _try_navigation() to also check for structured data in <script> tags (Next.js __NEXT_DATA__, Docusaurus docusaurus.config, etc.) and JSON-LD navigation. Add fallback: if nav selectors find <5 URLs but page has many <a> tags, expand search to all internal links within content area. Test against captured HTML from these 3 SPA sites.",
      "acceptance_criteria": [
        "Extracts navigation from __NEXT_DATA__ JSON in React.dev HTML — verified by test with captured react HTML fixture",
        "Falls back to content-area link extraction when nav selectors find <5 URLs — verified by test",
        "Finds >20 URLs from React.dev captured HTML (currently finds ~5 via nav) — verified by count assertion",
        "Finds >50 URLs from Tailwind captured HTML — verified by count assertion",
        "Does not regress on server-rendered sites (fastapi, kubernetes) — verified by regression test"
      ],
      "passes": false
    },
    {
      "group": "Full-Scrape-Capture",
      "feature": "Bulk HTML capture for all 10 reference doc-sets (full site)",
      "description": "Extend run_capture.py to capture ALL discovered URLs per site (not just 3-5). For each of the 10 reference doc-sets: run URL discovery, then capture every discovered page as .meta.json + .body.html. Store in tests/fixtures/real-world/{doc-name}/captured/. Add --max-pages-per-site flag (default: 50) to limit capture size. Track capture manifest with total pages captured per site. Target: 30-50 pages per major site (react, fastapi, tailwind, kubernetes, django), 10-20 for smaller sites.",
      "acceptance_criteria": [
        "Capture script accepts --max-pages-per-site N flag — verified by CLI test",
        "Each of the 10 doc-sets has captured/ directory with .meta.json + .body.html pairs — verified by directory scan",
        "Major sites (react, fastapi, tailwind, kubernetes, django) have >= 30 captured pages — verified by file count",
        "Smaller sites (hyperapp, rust-book) have >= 5 captured pages — verified by file count",
        "Capture manifest updated with actual page counts and capture timestamp — verified by JSON schema check"
      ],
      "passes": false
    },
    {
      "group": "Full-Scrape-Capture",
      "feature": "Full offline scrape test using all captured HTML",
      "description": "Create test_full_scrape_offline.py that runs ScraperEngine against ALL captured HTML files per site (not just 3-5). For each doc-set: start CaptureServer with all captured pages → run ScraperEngine.scrape_all() → verify output .md files. Compare scraped page count against captured page count. This tests the complete scraping pipeline offline with realistic data volume.",
      "acceptance_criteria": [
        "Test runs ScraperEngine for each of the 10 doc-sets with all captured pages — verified by pytest parametrize",
        "Scraper produces >= 80% of captured page count as .md output files — verified by count comparison",
        "No crashes or unhandled exceptions for any doc-set — verified by clean pytest exit",
        "Total scrape time per site is < 60 seconds — verified by timing assertion",
        "Output .md files are stored in tests/e2e/full_scrape_output/{doc-name}/ for MCP evaluation — verified by directory check"
      ],
      "passes": false
    },
    {
      "group": "Full-Scrape-Capture",
      "feature": "Capture deduplication and storage optimization",
      "description": "When capturing 30-50 pages per site, many pages share identical boilerplate (headers, footers, nav). Implement deduplication: store common HTML fragments once, reference them in meta.json. Also implement .body.html.gz compression for large files (>100KB). Add capture stats: total size, deduplicated size, compression ratio.",
      "acceptance_criteria": [
        "Large body files (>100KB) are stored as .body.html.gz — verified by checking file extension for kubernetes captures",
        "CaptureServer can serve both .body.html and .body.html.gz transparently — verified by HTTP response test",
        "Capture stats report total_size_bytes and compressed_size_bytes — verified by stats JSON output",
        "Storage savings >= 30% for sites with >20 pages — verified by comparing raw vs compressed totals",
        "Existing uncompressed captures still work (backward compatible) — verified by regression test with current fixtures"
      ],
      "passes": false
    },
    {
      "group": "Full-Scrape-Capture",
      "feature": "Captured HTML freshness check and re-capture workflow",
      "description": "Add metadata tracking for when each page was captured. Implement a freshness check: if captures are older than N days (default: 90), flag them for re-capture. Create a --refresh flag for the capture script that only re-captures stale pages. This ensures the test corpus stays current without re-downloading everything.",
      "acceptance_criteria": [
        "Each .meta.json has captured_at timestamp — verified by schema check on all meta files",
        "Freshness check reports stale captures (>90 days) — verified by test with mock old timestamp",
        "--refresh flag only re-captures stale pages, skips fresh ones — verified by test with mix of old/new captures",
        "Re-capture preserves the old file as .body.html.bak before overwriting — verified by backup file check",
        "Summary report shows: total captures, fresh, stale, re-captured — verified by report JSON output"
      ],
      "passes": false
    },
    {
      "group": "Full-Scrape-Capture",
      "feature": "Capture validation and integrity check",
      "description": "Add a validation step after capture: verify each .body.html is valid HTML (not error pages, not redirects, not empty). Check that Content-Type is text/html, status is 200, body has >1000 bytes, body contains <html> or <body> tag. Flag and quarantine invalid captures. Add a --validate flag to check existing captures without re-downloading.",
      "acceptance_criteria": [
        "Validation rejects captures with status != 200 — verified by test with 404 capture",
        "Validation rejects captures with body < 1000 bytes — verified by test with tiny capture",
        "Validation rejects captures without <html> or <body> tag — verified by test with JSON response",
        "--validate flag checks all existing captures and reports results — verified by running against current fixtures",
        "Invalid captures are moved to captured/quarantine/ subdirectory — verified by quarantine directory check"
      ],
      "passes": false
    },
    {
      "group": "Content-Extraction-Quality",
      "feature": "HTML residue audit and targeted cleanup for Tailwind (2,045 instances)",
      "description": "Tailwind has 2,045 HTML residue instances — the worst of all 10 doc-sets. Analyze the actual HTML tags leaking through: likely <div>, <span> with Tailwind utility classes, interactive code example containers, and color swatch elements. Create targeted cleanup rules in ContentCleaner for these patterns. Verify by re-running the markdown quality scorer before and after cleanup. Target: reduce Tailwind HTML residue from 2,045 to <100.",
      "acceptance_criteria": [
        "Audit identifies top 5 HTML tag patterns in Tailwind .md output — verified by audit script output",
        "ContentCleaner removes Tailwind-specific HTML residue (utility class divs, color swatches) — verified by unit test with Tailwind fixture",
        "Re-scraping Tailwind with updated cleaner reduces HTML residue from 2,045 to <100 — verified by quality scorer comparison",
        "React HTML residue (90) also reduced to <20 — verified by quality scorer",
        "No regression on clean sites (kubernetes, hyperapp-github) — verified by before/after comparison"
      ],
      "passes": false
    },
    {
      "group": "Content-Extraction-Quality",
      "feature": "Code block language tag correction in markdownify output",
      "description": "markdownify often assigns wrong language tags to code blocks: JavaScript becomes 'dockerfile', Python becomes 'dockerfile', etc. This happens because markdownify uses the CSS class of the <pre>/<code> element and sometimes misinterprets it. Fix by post-processing code blocks: detect actual language from content heuristics (import statements, syntax patterns) and correct the tag. Add a language_map for known CSS class → language mappings (e.g., 'language-js' → 'javascript', 'highlight-python' → 'python').",
      "acceptance_criteria": [
        "Language map covers top 15 CSS class patterns from real doc-sets — verified by map having >= 15 entries",
        "Post-processing corrects 'dockerfile' to correct language based on content — verified by test with JavaScript code tagged as dockerfile",
        "FastAPI code blocks correctly tagged as 'python' — verified by re-scraping FastAPI fixture and checking tags",
        "React code blocks correctly tagged as 'javascript'/'jsx' — verified by re-scraping React fixture",
        "Unknown languages default to 'text' rather than wrong tag — verified by unit test"
      ],
      "passes": false
    },
    {
      "group": "Content-Extraction-Quality",
      "feature": "Content selector accuracy test per site-type",
      "description": "The SiteAnalysis.content_selectors determine what HTML is extracted as documentation content. Test accuracy for each site-type: scrape a captured page, check if extracted content includes the main documentation text and excludes navigation/sidebar/footer. Build a ground-truth content test: for 5 captured pages, manually mark expected text snippets that MUST appear and text snippets that MUST NOT appear in output.",
      "acceptance_criteria": [
        "Ground-truth expectations for 5 pages (1 per site-type: mkdocs, docusaurus, hugo, sphinx, custom) — verified by fixture files with must_contain/must_not_contain arrays",
        "All must_contain strings appear in scraped .md output — verified by content check test",
        "No must_not_contain strings appear in scraped .md output — verified by content check test",
        "Content extraction includes code examples from all 5 test pages — verified by code block count > 0",
        "Content extraction excludes sidebar navigation text — verified by checking nav text absence"
      ],
      "passes": false
    },
    {
      "group": "Content-Extraction-Quality",
      "feature": "Encoding error elimination for rust-book (27 errors)",
      "description": "rust-book has 27 encoding errors (mojibake patterns). Investigate root cause: likely the scraper fetches with wrong encoding, or markdownify mishandles certain UTF-8 sequences. Fix by: improving charset detection in fetch_page(), adding BOM handling, and adding a post-processing encoding repair step in ContentCleaner. Verify fix by re-scraping rust-book and running encoding audit.",
      "acceptance_criteria": [
        "Root cause identified: document which specific characters/patterns cause the 27 errors — verified by audit report",
        "Fix implemented in fetch_page() or ContentCleaner — verified by code change",
        "Re-scraping rust-book produces 0 encoding errors — verified by encoding audit on new output",
        "Encoding repair does not corrupt valid UTF-8 content — verified by regression test on fastapi output",
        "fix_encoding_issues() in ContentCleaner handles all identified mojibake patterns — verified by unit test with each pattern"
      ],
      "passes": false
    },
    {
      "group": "Content-Extraction-Quality",
      "feature": "Heading hierarchy normalization across all site-types",
      "description": "Different doc sites use different heading hierarchies: some start at h1, some at h2, some skip levels (h1 → h3). The MCP MarkdownParser expects consistent heading levels for good TOC generation. Add a heading normalization step in ContentCleaner that ensures: each page starts with h1 (or h2 for sub-pages), no levels are skipped, and duplicate h1s are cleaned. Test against all 10 doc-set outputs.",
      "acceptance_criteria": [
        "Heading normalizer ensures no skipped levels (e.g., h1 → h3 without h2) — verified by unit test",
        "Pages with multiple h1 headings are cleaned to single h1 + h2s — verified by unit test",
        "Normalizer preserves original heading text — verified by content comparison",
        "All 10 doc-set outputs pass heading hierarchy validation after normalization — verified by batch test",
        "MCP MarkdownParser TOC generation improves (fewer orphan entries) — verified by TOC entry count comparison"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-Tuning",
      "feature": "TF-IDF scoring to replace raw term frequency",
      "description": "The current search uses raw term count (capped at 20) which treats common words the same as rare domain-specific terms. Implement TF-IDF (Term Frequency - Inverse Document Frequency) scoring in MarkdownParser.search(): build a document frequency map during buildIndex() that counts how many sections contain each word. During search, weight rare terms higher than common ones. This should dramatically improve precision for specific queries like 'ConfigMaps' vs generic ones like 'configuration'.",
      "acceptance_criteria": [
        "buildIndex() computes document frequency (DF) for each unique word across all sections — verified by checking DF map exists with >100 entries",
        "search() uses TF-IDF scoring: score = tf * log(N/df) instead of raw count — verified by unit test comparing old vs new scoring",
        "Query 'ConfigMaps' ranks kubernetes ConfigMaps section #1 (currently not) — verified by search result test",
        "Query 'authentication' still returns relevant results (no regression) — verified by regression test",
        "Search benchmark precision@1 improves from 0.35 to >= 0.50 across all 10 doc-sets — verified by re-running quality benchmark"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-Tuning",
      "feature": "Multi-word phrase matching with proximity scoring",
      "description": "Current search splits queries into individual terms and scores each independently. This means 'Docker deployment' matches any section mentioning 'Docker' OR 'deployment' separately. Add phrase matching: when query terms appear adjacent or within N words of each other in the content, add a proximity bonus. Exact phrase match gets highest boost. This improves relevance for multi-word queries.",
      "acceptance_criteria": [
        "Exact phrase match ('Docker deployment' as substring) gets 3x bonus over individual term matches — verified by unit test",
        "Proximity within 5 words gets 2x bonus — verified by unit test with crafted content",
        "Query 'OAuth2 authentication' ranks OAuth2 section higher than generic auth sections — verified by search test on fastapi",
        "Single-word queries are unaffected (no regression) — verified by regression test",
        "Search benchmark MRR improves from 0.35 to >= 0.50 — verified by re-running quality benchmark"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-Tuning",
      "feature": "Section hierarchy boost for parent-child relevance",
      "description": "When a section's parent heading matches the query, the child section should get a relevance boost. Example: query 'Pods' should rank sections under the 'Pods' h2 heading higher, even if those child sections don't mention 'Pods' directly. Currently, path matching gives 30 points but is the same for all path depths. Add a depth-weighted path boost: direct parent match = 25pts, grandparent = 15pts, great-grandparent = 10pts.",
      "acceptance_criteria": [
        "Direct parent title matching query term gets +25 bonus — verified by unit test with nested sections",
        "Grandparent matching gets +15 bonus — verified by unit test",
        "Child sections under 'Pods' h2 rank higher for query 'Pods lifecycle' — verified by kubernetes search test",
        "No regression for top-level section searches — verified by regression test",
        "Heading hierarchy data (path array) is correctly populated for all doc-sets — verified by index validation test"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-Tuning",
      "feature": "Stop word removal and query preprocessing",
      "description": "Queries like 'how to deploy with Docker' score 'how', 'to', 'deploy', 'with', 'Docker' equally. Common stop words ('how', 'to', 'with', 'the', 'a', 'is', etc.) dilute relevance scores. Add stop word filtering: remove common English stop words from queries before scoring. Also add query normalization: strip punctuation, collapse whitespace, handle camelCase splitting (e.g., 'useState' → 'use state').",
      "acceptance_criteria": [
        "Stop word list has >= 50 common English words (the, a, is, how, to, with, etc.) — verified by list length check",
        "Query 'how to deploy with Docker' is preprocessed to ['deploy', 'docker'] — verified by unit test",
        "camelCase splitting: 'useState' → ['use', 'state'] for search — verified by unit test",
        "Query 'What is a ConfigMap?' preprocessed to ['configmap'] — verified by unit test",
        "Search results for 'how to deploy with Docker' are same quality as 'deploy Docker' — verified by result comparison test"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-Tuning",
      "feature": "Search result diversity and file-level deduplication",
      "description": "When a large file has 50 sections all mentioning 'configuration', search results are dominated by that single file. Add file-level diversity: after scoring, ensure no more than 3 results from the same file in the top 10. Promote the highest-scoring section from each file, then fill remaining slots with next-best from other files. This gives users a broader view across the documentation.",
      "acceptance_criteria": [
        "Top 10 results contain sections from at least 3 different files (when available) — verified by file diversity test on kubernetes",
        "No more than 3 results from same file in top 10 — verified by count-per-file assertion",
        "Highest-scoring section per file is always included — verified by score ordering test",
        "File diversity does not apply when maxResults <= 3 — verified by small result set test",
        "Search benchmark overall score does not decrease — verified by before/after benchmark comparison"
      ],
      "passes": false
    },
    {
      "group": "Query-Suite-Calibration",
      "feature": "Manual ground-truth query annotations for 5 core doc-sets",
      "description": "The current query-suite.json uses fuzzy-matched expected_top_title which often maps to wrong headings. For 5 core doc-sets (react, fastapi, kubernetes, tailwind, golang), manually create ground-truth annotations: for each query, specify the exact section title AND section ID from the actual MCP index that should be the correct top result. Build an annotation tool that loads the MCP index, shows search results, and lets the annotator mark the correct answer. Store annotations in tests/fixtures/real-world/query-annotations/{doc-name}.json.",
      "acceptance_criteria": [
        "Annotation tool script that loads MCP index and presents search results for review — verified by script existence and --help output",
        "Each of 5 core doc-sets has annotation file with >= 8 annotated queries — verified by file existence and count",
        "Each annotation has: query, correct_section_id, correct_title, correct_file, relevance_grade (exact/partial/none) — verified by JSON schema validation",
        "Total >= 40 manually verified query-answer pairs — verified by counting across all annotation files",
        "Annotation format is compatible with eval-realworld.ts evaluation script — verified by integration test"
      ],
      "passes": false
    },
    {
      "group": "Query-Suite-Calibration",
      "feature": "Query type distribution across conceptual, API, code, and troubleshooting queries",
      "description": "Current query-suite.json has mostly conceptual queries ('React hooks', 'Kubernetes pods'). Real users also search for: API signatures ('useState parameters'), code patterns ('fetch data useEffect'), error messages ('CORS error'), and how-to queries ('deploy to production'). Rebalance the query suite to have 25% conceptual, 25% API/reference, 25% code patterns, 25% how-to/troubleshooting. Add query_type field to each query.",
      "acceptance_criteria": [
        "Each query has query_type field: 'conceptual', 'api_reference', 'code_pattern', or 'howto' — verified by schema check",
        "Distribution is within 20-30% per type (no type below 20%) — verified by count per type",
        "At least 15 queries per doc-set (up from current 5-10) — verified by per-doc count",
        "Total query count >= 100 — verified by total count",
        "New query types include API signatures, error messages, and code snippets — verified by content check"
      ],
      "passes": false
    },
    {
      "group": "Query-Suite-Calibration",
      "feature": "Automated query generation from section titles and code blocks",
      "description": "Manually writing 100+ queries is slow and biased. Create a script that auto-generates candidate queries from the MCP index: extract section titles as 'conceptual' queries, extract function/class names from code blocks as 'api_reference' queries, extract error strings as 'troubleshooting' queries. Human reviews and filters the candidates. The correct answer is the section the query was generated from.",
      "acceptance_criteria": [
        "Generator script produces candidate queries from section titles — verified by script output with >= 50 candidates per doc-set",
        "Code block extraction finds function names, class names, API endpoints — verified by regex pattern tests",
        "Generated queries have automatic correct_section_id (the source section) — verified by ID validity check",
        "Human review workflow: script outputs candidates to review file, human marks keep/discard — verified by review file format",
        "At least 20 auto-generated queries pass human review per core doc-set — verified by approved count"
      ],
      "passes": false
    },
    {
      "group": "Query-Suite-Calibration",
      "feature": "Cross-doc-set query deduplication and difficulty grading",
      "description": "Some queries appear across multiple doc-sets with different expected answers (e.g., 'authentication' appears in fastapi, django, synthflow). Ensure each query is unique per doc-set and add difficulty grading: 'easy' (exact title match exists), 'medium' (topic exists but title differs), 'hard' (requires semantic understanding). Report search benchmark broken down by difficulty level.",
      "acceptance_criteria": [
        "No duplicate queries within the same doc-set — verified by uniqueness check",
        "Each query has difficulty field: 'easy', 'medium', or 'hard' — verified by schema check",
        "Difficulty distribution: ~30% easy, ~40% medium, ~30% hard — verified by count per difficulty",
        "Search benchmark reports precision broken down by difficulty — verified by benchmark output format",
        "Easy queries achieve precision@1 >= 0.80 — verified by benchmark result"
      ],
      "passes": false
    },
    {
      "group": "Query-Suite-Calibration",
      "feature": "Django English-only re-scrape and query suite fix",
      "description": "Django's AppData contains Greek translations (v1 scraped /el/ URLs). Re-scrape Django with the correct English start URL (https://docs.djangoproject.com/en/stable/) and locale filter. Update the query-suite with proper English Django queries targeting real section titles like 'Models', 'Views', 'Templates', 'Forms', 'Admin'. Verify search works correctly on the English content.",
      "acceptance_criteria": [
        "Django re-scraped from /en/stable/ with locale_filter='en' — verified by config.json start_url check",
        "All Django .md files contain English content (no Greek characters) — verified by encoding scan",
        "Django query-suite has 10 queries with manually verified English expected_top_title — verified by annotation file",
        "Django search precision@1 >= 0.50 (up from 0.00) — verified by search benchmark",
        "Django .md file count >= 40 (similar to v1's 60 but all English) — verified by file count"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Hardening",
      "feature": "Full-site E2E test: capture → discover → scrape → index → search → score",
      "description": "Create the definitive E2E test that runs the COMPLETE pipeline for each of the 10 reference doc-sets: (1) serve all captured HTML via CaptureServer, (2) run URLDiscovery against the server, (3) run ScraperEngine.scrape_all(), (4) run MarkdownParser.buildIndex(), (5) run search queries from query-suite, (6) compute quality scores. Each step feeds into the next. A single test run produces a complete quality dashboard. This replaces the current piecemeal tests.",
      "acceptance_criteria": [
        "Single pytest command runs full pipeline for all 10 doc-sets — verified by pytest tests/e2e/test_pipeline_full.py -v",
        "Each doc-set pipeline step is timed and logged — verified by timing output per step",
        "Pipeline produces quality-dashboard.json with per-doc-set and aggregate scores — verified by JSON output",
        "Any doc-set with overall_score < 0.50 is flagged as FAILING — verified by status field in dashboard",
        "Total pipeline runtime < 5 minutes for all 10 doc-sets — verified by total time assertion"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Hardening",
      "feature": "Regression detection via golden output comparison",
      "description": "Store the current scraper .md output for 5 key pages as golden baselines. On each E2E run, compare new output against golden baselines using diff. Flag regressions: lost headings, lost code blocks, new HTML residue, content truncation, encoding errors. This catches unintended side effects of code changes. Use semantic diff (ignore whitespace changes, focus on structure).",
      "acceptance_criteria": [
        "Golden baselines stored for 5 pages (1 per core site: react, fastapi, kubernetes, tailwind, golang) — verified by file existence",
        "Diff tool compares: heading count, code block count, content length, encoding errors — verified by diff output format",
        "Regression detected when heading count drops by >10% — verified by test with intentionally broken cleaner",
        "Regression detected when new HTML tags appear in output — verified by test with broken selector",
        "Clean run (no code changes) produces 0 regressions — verified by baseline comparison"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Hardening",
      "feature": "Quality score trend tracking across runs",
      "description": "Store quality dashboard results from each test run in a history file (tests/e2e/quality-history.jsonl, one JSON line per run). Add trend analysis: compare current run against last 3 runs, flag improvements and regressions per dimension. Display a summary showing score trajectory. This enables tracking whether code changes improve or degrade quality over time.",
      "acceptance_criteria": [
        "Each benchmark run appends results to quality-history.jsonl — verified by line count increasing after run",
        "Trend report compares current vs previous run per dimension — verified by trend output format",
        "Improvements (score increase >= 0.05) are highlighted as IMPROVED — verified by test with mock history",
        "Regressions (score decrease >= 0.05) are highlighted as REGRESSED — verified by test with mock history",
        "History file is capped at 50 entries (oldest pruned) — verified by max line count check"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Hardening",
      "feature": "Error resilience test: malformed HTML, timeouts, encoding edge cases",
      "description": "The scraper should handle real-world edge cases gracefully: malformed HTML (unclosed tags), timeout responses, binary content served as text/html, enormous pages (>5MB), empty responses, redirect chains, and mixed encoding (UTF-8 BOM, Latin-1 declared as UTF-8). Create fixture files for each edge case and verify the scraper produces reasonable output (or skips gracefully) rather than crashing.",
      "acceptance_criteria": [
        "Scraper handles malformed HTML (unclosed tags) without crash — verified by test with malformed fixture",
        "Scraper handles empty response body gracefully (skips page) — verified by test with empty fixture",
        "Scraper handles >5MB page by truncating (not OOM) — verified by test with large fixture",
        "Scraper handles Latin-1 content declared as UTF-8 — verified by test with mismatched encoding fixture",
        "All edge case tests complete in < 10 seconds total — verified by timing assertion"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Hardening",
      "feature": "CI-ready test runner with quality gate enforcement",
      "description": "Create a CI-compatible test runner script (run_quality_gate.py) that: (1) runs all E2E tests, (2) generates quality dashboard, (3) compares against minimum quality thresholds, (4) exits with non-zero code if any threshold is violated. Thresholds: avg_markdown_score >= 0.80, avg_search_score >= 0.50, avg_overall_score >= 0.65. Include GitHub Actions workflow YAML for automated quality gate checks on PRs.",
      "acceptance_criteria": [
        "run_quality_gate.py runs all E2E tests and produces dashboard — verified by script execution",
        "Script exits 0 when all thresholds met — verified by test with good scores",
        "Script exits 1 with clear error when markdown score < 0.80 — verified by test with mock low score",
        "Script exits 1 with clear error when search score < 0.50 — verified by test with mock low score",
        "GitHub Actions workflow YAML (.github/workflows/quality-gate.yml) triggers on PR — verified by YAML schema validation"
      ],
      "passes": false
    }
  ]
}