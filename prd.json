{
  "project_meta": {
    "name": "anydocs-mcp-real-world-quality",
    "version": "2.0.0",
    "ralph_type": "opencode",
    "opencode_session_id": "real-world-quality-upgrade-v2"
  },
  "backlog": [
    {
      "group": "Capture-Replay-Infrastructure",
      "feature": "ResponseCapture class for recording HTTP responses",
      "description": "Create a ResponseCapture class in scraper/response_capture.py that fetches a URL and stores the complete HTTP response (status code, headers dict, body bytes) as a JSON metadata file + raw HTML body file pair. Each captured response is stored under tests/fixtures/real-world/{doc-name}/{url-slug}.meta.json and {url-slug}.body.html. The url-slug is derived from the URL path to keep filenames human-readable. Include a CLI entry point: python -m response_capture capture <url> <doc-name>.",
      "acceptance_criteria": [
        "ResponseCapture.capture(url) returns a CapturedResponse dataclass with status, headers, body, url — verify: unit test with mocked requests",
        "ResponseCapture.save(captured, doc_name) writes .meta.json (status, headers, original_url, captured_at) + .body.html — verify: file existence check in test",
        "URL-to-slug conversion produces readable filenames: https://react.dev/learn/state → react-dev-learn-state — verify: unit test with 5 URL examples",
        "CLI entry point captures a real URL when run manually (not tested in CI) — verify: code inspection of __main__ block",
        "CapturedResponse.load(doc_name, slug) reconstructs the response from disk — verify: round-trip test save→load"
      ],
      "passes": false
    },
    {
      "group": "Capture-Replay-Infrastructure",
      "feature": "Capture script for 10 reference doc-sets",
      "description": "Create a capture manifest file tests/fixtures/real-world/capture-manifest.json listing the 10 reference doc-sets with 3-5 representative URLs each (50 URLs total). Each entry has: doc_name, urls[], site_type (mkdocs/sphinx/docusaurus/hugo/github/custom). Include a runner script that iterates the manifest and captures all URLs using ResponseCapture. The 10 doc-sets are: react, fastapi, tailwind, kubernetes, django, hyperapp-github, onoffice, synthflow, golang, rust-book. URLs should cover: landing page, tutorial/guide page, API reference page, page with code examples, edge case page (very small or very large).",
      "acceptance_criteria": [
        "capture-manifest.json contains exactly 10 doc-sets with 3-5 URLs each — verify: JSON schema validation in test",
        "Each manifest entry has doc_name, urls (array of {url, page_type}), site_type — verify: test loads and validates structure",
        "Runner script capture_all.py iterates manifest and invokes ResponseCapture per URL — verify: code inspection",
        "At least 3 doc-sets have been actually captured (react, fastapi, hyperapp-github) with real responses on disk — verify: fixture files exist in tests/fixtures/real-world/",
        "Captured HTML files are under 1 MB each (truncate oversized responses) — verify: file size assertion in test"
      ],
      "passes": false
    },
    {
      "group": "Capture-Replay-Infrastructure",
      "feature": "FixtureHTTPServer mock server for replaying captured responses",
      "description": "Create a FixtureHTTPServer class in scraper/tests/fixture_server.py that starts a local HTTP server (http.server based) which replays captured responses from the fixtures directory. Given a request to http://localhost:{port}/{doc-name}/path, it looks up the matching .meta.json + .body.html fixture and returns the stored status code, headers, and body. Provide a pytest fixture @pytest.fixture that starts/stops the server and yields the base URL. The server must support concurrent requests (ThreadingMixIn).",
      "acceptance_criteria": [
        "FixtureHTTPServer(fixtures_dir).start() starts on a random free port, returns base_url string — verify: test connects to returned URL",
        "GET /{doc-name}/{path} returns the captured response body with correct Content-Type header — verify: test requests a known fixture and asserts body content",
        "Returns 404 for URLs without a matching fixture — verify: negative test",
        "Pytest fixture mock_http_server yields a started server and stops it in teardown — verify: test uses fixture without manual start/stop",
        "Server handles 10 concurrent requests without deadlock (ThreadingHTTPServer) — verify: test sends 10 parallel requests with concurrent.futures",
        "FixtureHTTPServer.get_rewritten_url(original_url) rewrites e.g. https://react.dev/learn/state to http://localhost:{port}/react/learn/state — verify: unit test"
      ],
      "passes": false
    },
    {
      "group": "Capture-Replay-Infrastructure",
      "feature": "Golden markdown output snapshots from real scraped docs",
      "description": "For each of the 10 reference doc-sets, copy the actual scraped markdown output from %APPDATA%/AnyDocsMCP/docs/{doc-name}/v*/  into tests/fixtures/real-world/{doc-name}/golden/. Select 1-2 representative markdown files per doc-set (the most content-rich ones, not stubs). These golden files represent the CURRENT scraper output quality — including all existing defects (encoding errors, UI artifacts). They serve as the baseline: future improvements must produce output that is BETTER than (or equal to) the golden files, never worse. Create a quality_baseline.json that records per-golden-file: encoding_error_count, artifact_count, heading_count, code_block_count, total_chars.",
      "acceptance_criteria": [
        "tests/fixtures/real-world/{doc-name}/golden/ contains 1-2 .md files per doc-set (10-20 golden files total) — verify: fixture file count assertion",
        "quality_baseline.json contains per-file metrics: encoding_errors, artifacts, headings, code_blocks, char_count — verify: JSON loads and all fields present",
        "Golden files are actual copies from the real scraped output, not synthetic — verify: files contain real doc content (spot-check react golden for JSX code)",
        "A script generate_baseline.py scans golden files and regenerates quality_baseline.json — verify: running script produces valid JSON matching file count",
        "Golden files total size is under 5 MB — verify: cumulative file size check"
      ],
      "passes": false
    },
    {
      "group": "Capture-Replay-Infrastructure",
      "feature": "Existing scraped docs as MCP search test corpus",
      "description": "Copy a curated subset of the already-scraped markdown docs into tests/fixtures/real-world/{doc-name}/mcp-corpus/ for use as MarkdownParser input in vitest tests. Select 3-5 representative .md files per doc-set from the existing scrape output. These files are fed directly to MarkdownParser (no scraping needed) to test search, indexing, overview, and TOC generation against real content. Update the vitest config to make this path available.",
      "acceptance_criteria": [
        "tests/fixtures/real-world/{doc-name}/mcp-corpus/ contains 3-5 .md files per doc-set for at least 5 doc-sets — verify: file count",
        "MarkdownParser can buildIndex() on each mcp-corpus directory without errors — verify: vitest test iterates all corpus dirs and calls buildIndex()",
        "Index contains >10 sections per corpus (real docs are content-rich) — verify: assertion on allSections.length",
        "Fixture total size is under 10 MB — verify: cumulative size check",
        "A README in tests/fixtures/real-world/ documents the fixture structure and how to refresh — verify: file exists"
      ],
      "passes": false
    },
    {
      "group": "Encoding-Quality",
      "feature": "Encoding audit script that categorizes all encoding defects",
      "description": "Create scraper/encoding_audit.py that scans a directory of markdown files and produces a structured JSON report of all encoding defects found. Categories: mojibake (â€™, â€œ, â€, etc.), broken-latin (Â followed by non-ASCII), permalink-anchors (¶ and [¶](#...) patterns in headings), html-entities (leftover &amp; &lt; etc.). The report groups defects by category, lists affected files, and counts occurrences. Run it against the golden fixtures and against the full %APPDATA% docs to establish the baseline. Output: encoding-audit-report.json.",
      "acceptance_criteria": [
        "encoding_audit.scan_directory(path) returns AuditReport with categories: mojibake, broken_latin, permalink_anchors, html_entities — verify: unit test with crafted fixture file containing all 4 categories",
        "Each category entry has: pattern_matched, file_path, line_number, context (±20 chars around match) — verify: test asserts context field is populated",
        "Report includes summary: {total_files, affected_files, defects_by_category: {mojibake: N, broken_latin: N, ...}} — verify: test asserts summary matches expected counts on fixture",
        "Running against tests/fixtures/real-world golden files produces a non-empty report (baseline has known defects) — verify: integration test",
        "CLI: python -m encoding_audit <directory> [--output report.json] — verify: code inspection"
      ],
      "passes": false
    },
    {
      "group": "Encoding-Quality",
      "feature": "Permalink anchor removal from headings",
      "description": "Extend ContentCleaner with a remove_permalink_anchors() method that strips permalink markers from markdown headings. Patterns to handle: (1) Heading text followed by ¶ character: '## Security¶' → '## Security', (2) Heading text with markdown permalink: '## Security[¶](#security \"Permanent link\")' → '## Security', (3) Heading with just anchor link: '## Security[](#security)' → '## Security'. Must preserve heading level and any inline code in the heading. Test against real fastapi and django heading patterns extracted from golden fixtures.",
      "acceptance_criteria": [
        "Removes trailing ¶ from headings: '## Security¶' → '## Security' — verify: unit test",
        "Removes [¶](#anchor \"Permanent link\") patterns: '## OAuth2[¶](#oauth2 \"Permanent link\")' → '## OAuth2' — verify: unit test with fastapi-style heading",
        "Removes [](#anchor) patterns: '### OpenID[](#openid)' → '### OpenID' — verify: unit test",
        "Preserves inline code in headings: '## Using `async`[¶](#using-async)' → '## Using `async`' — verify: unit test",
        "Preserves non-heading lines containing ¶ (if any) — verify: negative test",
        "Running clean() on fastapi golden fixture reduces permalink-anchor count to 0 — verify: integration test comparing before/after audit report"
      ],
      "passes": false
    },
    {
      "group": "Encoding-Quality",
      "feature": "Comprehensive UTF-8 mojibake repair mapping",
      "description": "Extend ContentCleaner with a fix_mojibake() method that systematically repairs common UTF-8 double-encoding (mojibake) artifacts. Build a mapping table of the most frequent patterns found in the real scraped docs: curly quotes, em/en dashes, orphan Latin supplement chars, accented letters, etc. The mapping must be applied BEFORE other cleaning steps to ensure correct text for downstream processing. Include a confidence check: only apply replacements when the surrounding context confirms mojibake (avoid false positives).",
      "acceptance_criteria": [
        "Mapping table covers at least 15 mojibake patterns found in real data — verify: test asserts mapping dict length >= 15",
        "Repairs mojibake right single quote to proper Unicode — verify: unit test",
        "Repairs mojibake left/right double quotes to proper Unicode — verify: unit test",
        "Repairs mojibake em dash and en dash to proper Unicode — verify: unit test",
        "Removes orphan Latin supplement prefix before non-ASCII chars — verify: unit test",
        "Does NOT corrupt legitimate text containing similar byte sequences — verify: negative test with word chateau unchanged",
        "Running fix_mojibake on fastapi golden fixture reduces mojibake count by >90% — verify: before/after audit",
        "fix_mojibake is called as first step in clean() pipeline — verify: code inspection of clean() method order"
      ],
      "passes": false
    },
    {
      "group": "Encoding-Quality",
      "feature": "Encoding regression tests with real fixture data",
      "description": "Create scraper/tests/test_encoding_realworld.py with regression tests that load golden fixture files from tests/fixtures/real-world/{doc-name}/golden/, run them through ContentCleaner.clean(), and assert that the output has zero encoding defects. Use the encoding_audit module to scan the cleaned output. This creates an automated regression gate: any future change to the cleaner that reintroduces encoding defects will fail these tests. Test at minimum against: fastapi (100% affected), django (95%), react (100%), kubernetes (61%).",
      "acceptance_criteria": [
        "Test loads golden .md files from at least 4 doc-sets (fastapi, django, react, kubernetes) — verify: parametrized test with doc-name parameter",
        "Each golden file is processed through ContentCleaner.clean() — verify: test calls clean() on file content",
        "Cleaned output is scanned with encoding_audit for mojibake and permalink_anchors — verify: test calls audit.scan_text()",
        "Assert mojibake_count == 0 and permalink_anchor_count == 0 on cleaned output — verify: assertion in test",
        "Test produces a human-readable diff showing what was changed (for review) — verify: test logs before/after line count delta",
        "All 4+ doc-set tests pass — verify: pytest passes"
      ],
      "passes": false
    },
    {
      "group": "Encoding-Quality",
      "feature": "Response charset detection and conversion in fetch_page",
      "description": "Enhance ScraperEngine.fetch_page() to properly detect and handle response character encoding. Currently relies on requests' default encoding detection, which fails for pages that declare charset in HTML meta tags but not in HTTP headers. Add: (1) Check Content-Type header for charset parameter, (2) Fall back to HTML meta charset detection, (3) If detected charset is not UTF-8, convert response.text using the correct encoding. This prevents mojibake at the source rather than fixing it after the fact.",
      "acceptance_criteria": [
        "Extracts charset from Content-Type header: 'text/html; charset=utf-8' → 'utf-8' — verify: unit test with mocked response",
        "Falls back to HTML meta charset detection if header has no charset — verify: unit test with HTML fixture containing meta tag",
        "Falls back to legacy meta http-equiv Content-Type charset detection — verify: unit test with legacy meta tag",
        "If charset is not utf-8, sets response.encoding before accessing response.text — verify: unit test with mocked latin-1 response",
        "Does not change behavior for responses that are already UTF-8 — verify: existing fetch_page tests still pass",
        "Tested against captured fixtures from FixtureHTTPServer — verify: integration test with mock server serving a non-UTF-8 fixture"
      ],
      "passes": false
    },
    {
      "group": "Artifact-Cleanup-Profiles",
      "feature": "Pattern discovery script for automated artifact detection",
      "description": "Create scraper/pattern_discovery.py that scans a directory of scraped markdown files and automatically identifies recurring non-content patterns. Uses frequency analysis: lines/phrases that appear in >30% of files in a doc-set but carry no semantic value (navigation, UI chrome, boilerplate). Outputs a ranked list of candidate artifact patterns with occurrence count, sample context, and suggested regex. This enables data-driven pattern identification instead of manually guessing what to clean.",
      "acceptance_criteria": [
        "pattern_discovery.scan_directory(path) returns list of PatternCandidate(pattern, count, percentage, sample_contexts) — verify: unit test with crafted fixture dir containing repeated boilerplate",
        "Identifies lines appearing in >30% of files as candidates — verify: test with 10 files where 5 share a common line",
        "Filters out legitimate repeated content (headings like Introduction are OK) vs boilerplate (Show more, On this page) — verify: test with mix of both",
        "Running against real onoffice golden fixtures (highest file count) produces >5 candidate patterns — verify: integration test",
        "Output includes suggested regex for each pattern — verify: test asserts regex field is present and compilable",
        "CLI: python -m pattern_discovery <directory> [--threshold 0.3] — verify: code inspection"
      ],
      "passes": false
    },
    {
      "group": "Artifact-Cleanup-Profiles",
      "feature": "Site-type profile system for ContentCleaner",
      "description": "Extend ContentCleaner to accept an optional site_type parameter (mkdocs, sphinx, docusaurus, hugo, github, custom) that loads a predefined set of artifact patterns specific to that documentation framework. Each profile is a list of regex patterns and replacement rules. The clean() method applies the base universal patterns PLUS the site-specific profile patterns. Profiles are stored as a PROFILES dict inside content_cleaner.py. The scraper auto-detects site_type from SiteAnalysis or URL patterns and passes it to the cleaner.",
      "acceptance_criteria": [
        "ContentCleaner(site_type='mkdocs') loads MkDocs-specific patterns — verify: test asserts profile patterns are loaded",
        "ContentCleaner(site_type=None) uses only universal patterns (backward compatible) — verify: existing tests still pass",
        "PROFILES dict contains entries for at least: mkdocs, sphinx, docusaurus, hugo — verify: test iterates PROFILES keys",
        "clean() applies universal patterns first, then site-specific patterns — verify: test with content matching both universal and site-specific artifacts",
        "ScraperEngine passes site_type from config/analysis to ContentCleaner — verify: integration test or code inspection",
        "Unknown site_type logs warning and falls back to universal-only — verify: test with site_type='unknown'"
      ],
      "passes": false
    },
    {
      "group": "Artifact-Cleanup-Profiles",
      "feature": "MkDocs and Sphinx cleanup profiles",
      "description": "Create cleanup profiles for MkDocs-based sites (fastapi, kubernetes) and Sphinx-based sites (django, python3). MkDocs profile handles: permalink anchors in headings, 'Edit on GitHub' links, 'Last updated' timestamps, admonition syntax artifacts (!!!, ???), tab-set artifacts. Sphinx profile handles: 'Changed in version X.Y', 'New in version X.Y', 'Deprecated since version X.Y' badges, note/warning/tip box markers, '(source)' links, cross-reference artifacts like :ref: and :doc:. Test each profile against the corresponding golden fixtures.",
      "acceptance_criteria": [
        "MkDocs profile removes 'Edit on GitHub/GitLab' link lines — verify: unit test",
        "MkDocs profile removes admonition markers (!!!, ???) leaving content — verify: unit test with '!!! note' block",
        "MkDocs profile removes 'Last updated:' lines — verify: unit test",
        "Sphinx profile preserves 'Changed in version' as inline note but removes badge markup — verify: unit test",
        "Sphinx profile removes '(source)' links after class/function names — verify: unit test",
        "MkDocs profile tested against fastapi golden fixture: artifact count reduced by >80% — verify: before/after comparison",
        "Sphinx profile tested against django golden fixture: artifact count reduced by >80% — verify: before/after comparison"
      ],
      "passes": false
    },
    {
      "group": "Artifact-Cleanup-Profiles",
      "feature": "Docusaurus and Hugo cleanup profiles",
      "description": "Create cleanup profiles for Docusaurus-based sites (react) and Hugo-based sites (kubernetes). Docusaurus profile handles: CodeSandbox/StackBlitz embed artifacts, 'Show more/Show less' toggle text, tab switcher remnants (TabItem markers), interactive playground artifacts (ReloadClear, sp-pre-placeholder), 'tip/info/warning/danger' admonition markers. Hugo profile handles: 'On this page' sidebar TOC remnants, breadcrumb trails, 'Last modified' timestamps, 'Edit this page' links, 'Feedback: Was this page helpful?' sections. Test each against golden fixtures.",
      "acceptance_criteria": [
        "Docusaurus profile removes 'Show more' and 'Show less' standalone lines — verify: unit test",
        "Docusaurus profile removes CodeSandbox fork links and ReloadClear artifacts — verify: unit test",
        "Docusaurus profile removes tab switcher remnants — verify: unit test",
        "Hugo profile removes 'On this page' followed by anchor-link lists — verify: unit test",
        "Hugo profile removes 'Was this page helpful?' feedback sections — verify: unit test",
        "Hugo profile removes breadcrumb lines (Home > Docs > Section > Page) — verify: unit test",
        "Docusaurus profile against react golden: artifact count reduced by >80% — verify: before/after",
        "Hugo profile against kubernetes golden: 'On this page' occurrences reduced to 0 — verify: grep count"
      ],
      "passes": false
    },
    {
      "group": "Artifact-Cleanup-Profiles",
      "feature": "Cookie banner and generic SPA artifact cleanup",
      "description": "Create a 'generic-spa' cleanup profile for sites that don't fit standard doc frameworks (synthflow, onoffice, siliconforest). Handles: cookie consent banner remnants (Accept/Reject/Cookie Policy text), newsletter signup artifacts, social media share buttons text, 'Back to top' links, footer boilerplate (Copyright, Terms, Privacy Policy), loading spinner text, 'Search docs...' placeholder text. Also handles the most common cross-framework artifacts found by the pattern discovery script. This profile is applied as a fallback when no specific framework profile matches.",
      "acceptance_criteria": [
        "Removes cookie consent text blocks (lines containing 'cookie', 'consent', 'accept all' in non-content context) — verify: unit test",
        "Removes newsletter signup blocks ('Subscribe', 'Enter your email') — verify: unit test",
        "Removes footer boilerplate (Copyright lines, Terms of Service links) — verify: unit test",
        "Removes 'Back to top' link lines — verify: unit test",
        "Does NOT remove legitimate content mentioning cookies (e.g., HTTP cookie documentation) — verify: negative test with content about browser cookies",
        "Applied to synthflow golden fixture: cookie artifact count drops to 0 — verify: integration test",
        "generic-spa profile is used as default when site_type is 'custom' or unknown — verify: test"
      ],
      "passes": false
    },
    {
      "group": "Content-Sizing-Chunking",
      "feature": "File size guardrails with configurable max size and auto-split",
      "description": "Add a max_file_size_kb parameter to ScraperEngine (default: 500 KB). When a scraped output file exceeds this limit, split it into numbered parts ({group}-part1.md, {group}-part2.md, etc.) at heading boundaries. The split algorithm walks the markdown content, accumulates sections until the size threshold is approaching, then cuts at the nearest h2 boundary. Log a warning when splitting occurs. This prevents monster files like nodejs (3 MB avg) and nextjs (2.3 MB avg) from being unusable.",
      "acceptance_criteria": [
        "ScraperEngine accepts max_file_size_kb parameter (default 500) — verify: constructor test",
        "Files under the limit are not modified — verify: test with small file",
        "Files over the limit are split at h2 heading boundaries — verify: test with 600 KB fixture, assert 2 output files",
        "Split files have correct numbering: {group}-part1.md, {group}-part2.md — verify: filename assertion",
        "Each split file retains the document header (title, source info) — verify: test asserts header present in all parts",
        "Warning is logged when splitting occurs — verify: caplog assertion in test",
        "Tested against real nodejs or nextjs golden fixture (>1 MB) — verify: integration test produces multiple smaller files"
      ],
      "passes": false
    },
    {
      "group": "Content-Sizing-Chunking",
      "feature": "Stub page detection and filtering",
      "description": "Add stub page detection to ScraperEngine._scrape_single_page(). A page is a stub if its cleaned content is under a configurable threshold (default: 200 chars) AND it contains no code blocks AND no more than 1 heading. Stub pages are logged and excluded from the output (not written to disk). Optionally, stub pages can be collected into a {group}-stubs.md file if there are >3 stubs in the same group. This addresses the issue of pages like tailwind docs-installation.md (370 bytes with just a title) and fastapi about.md (220 bytes) polluting the output.",
      "acceptance_criteria": [
        "Pages with <200 chars, 0 code blocks, and <=1 heading are classified as stubs — verify: unit test with stub content",
        "Pages with <200 chars BUT containing code blocks are NOT classified as stubs — verify: negative test",
        "Stub pages are not written to the main output file — verify: test asserts stub content absent from output",
        "Stub pages are logged with URL and char count — verify: caplog/stderr assertion",
        "If >3 stubs in same group, they are collected into {group}-stubs.md — verify: test with 4 stub pages",
        "min_content_chars is configurable via ScraperEngine parameter — verify: constructor test",
        "Tested against real tailwind doc-set: docs-installation.md (370B) is classified as stub — verify: integration test"
      ],
      "passes": false
    },
    {
      "group": "Content-Sizing-Chunking",
      "feature": "Reference table compression for oversized utility docs",
      "description": "Add a compress_reference_tables() method to ContentCleaner that detects and compresses very large markdown tables (>100 rows). For tables where all rows follow the same pattern (like Tailwind CSS utility classes), keep the first 10 rows as examples, add a summary row showing the pattern and total count, and collapse the rest. The compression is opt-in via a compress_tables parameter on clean(). This addresses Tailwind border-color.md being 264 KB of just color variant tables. The compressed version should still be searchable (keep class name prefixes) but dramatically smaller.",
      "acceptance_criteria": [
        "Tables with >100 rows are detected — verify: unit test with 150-row table",
        "Tables with <100 rows are not modified — verify: negative test with 50-row table",
        "Compressed table keeps first 10 rows + summary row — verify: unit test asserting 11 data rows in output",
        "Summary row includes original row count and pattern description — verify: test asserts summary content",
        "compress_tables=False (default) leaves tables unchanged — verify: backward compatibility test",
        "Tested against real Tailwind border-color.md golden fixture: file size reduced by >70% — verify: before/after size comparison",
        "Compressed content still contains searchable class name prefixes (border-red, border-blue, etc.) — verify: test asserts key prefixes present"
      ],
      "passes": false
    },
    {
      "group": "Content-Sizing-Chunking",
      "feature": "Duplicate version detection across doc-set versions",
      "description": "Create scraper/duplicate_detector.py with a detect_duplicate_versions() function that compares multiple version directories of the same doc-set (e.g., tailwind/v1, v2, v3, v4) and reports which versions are identical or near-identical. Uses content hashing at the file level: if >80% of files have identical hashes between two versions, they are flagged as duplicates. This helps users avoid storing redundant data (tailwind has 4 nearly identical versions). The detector produces a JSON report and optionally a CLI command to prune duplicates.",
      "acceptance_criteria": [
        "detect_duplicate_versions(doc_path) returns list of DuplicateGroup(versions, similarity_pct, identical_files) — verify: unit test with 3 versions, 2 identical",
        "Versions with >80% identical file hashes are grouped as duplicates — verify: test with controlled fixture",
        "Versions with <50% identical files are NOT flagged — verify: negative test",
        "Report includes per-version: file_count, total_size, unique_files — verify: test asserts all fields",
        "CLI: python -m duplicate_detector <doc-root> [--threshold 0.8] — verify: code inspection",
        "Tested against real tailwind doc-set (4 versions): correctly identifies duplicate versions — verify: integration test"
      ],
      "passes": false
    },
    {
      "group": "Content-Sizing-Chunking",
      "feature": "Content quality score per scraped page",
      "description": "Add a compute_page_quality_score() method to ContentCleaner that evaluates the quality of a cleaned markdown page and returns a score from 0.0 to 1.0. Scoring factors: heading_structure (proper hierarchy, not all h1), code_block_ratio (pages with code examples score higher for API docs), artifact_residue (remaining UI artifacts penalize score), content_density (meaningful text vs boilerplate ratio), encoding_health (presence of mojibake penalizes). The score is logged per page during scraping and aggregated into the final quality metrics. Pages scoring below 0.3 trigger a warning.",
      "acceptance_criteria": [
        "compute_page_quality_score(content) returns float 0.0-1.0 — verify: unit test",
        "Clean page with good headings and code blocks scores >0.7 — verify: test with well-structured fixture",
        "Page with many artifacts and no headings scores <0.3 — verify: test with artifact-heavy fixture",
        "Page with encoding errors scores lower than same page without — verify: comparative test",
        "Empty or near-empty page scores 0.0 — verify: edge case test",
        "Score is logged during _scrape_single_page — verify: code inspection or caplog test",
        "Pages below 0.3 trigger a warning log — verify: caplog assertion",
        "Average quality score is included in scrape_all return metrics — verify: test on scrape output"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-RealData",
      "feature": "Real-world query suite with 100 curated queries across 10 doc-sets",
      "description": "Create tests/fixtures/real-world/query-suite.json containing 100 hand-curated search queries (10 per reference doc-set). Each query entry has: doc_name, query (natural language), expected_top_title (the section title that SHOULD rank #1), expected_file (which .md file it should come from), query_type (concept, api, code-example, troubleshooting, tutorial). Queries should be realistic: what a developer would actually ask while using the docs. Mix of single-term, multi-term, exact-match, and fuzzy queries. This is the ground truth for measuring search quality improvements.",
      "acceptance_criteria": [
        "query-suite.json contains exactly 100 entries — verify: JSON load and length assertion",
        "Each entry has fields: doc_name, query, expected_top_title, expected_file, query_type — verify: schema validation test",
        "At least 10 queries per doc-set (react, fastapi, tailwind, kubernetes, django, hyperapp-github, onoffice, synthflow, golang, rust-book) — verify: group-by count",
        "Query types are distributed: at least 15 concept, 15 api, 15 code-example, 10 troubleshooting, 10 tutorial — verify: type count assertion",
        "Each expected_top_title actually exists in the corresponding mcp-corpus fixture — verify: cross-reference test loads corpus and checks title exists",
        "Queries are realistic developer questions, not keyword stuffing — verify: manual review / code inspection"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-RealData",
      "feature": "Multi-corpus search benchmark with precision and MRR metrics",
      "description": "Create mcp-server/src/__tests__/realworld-search.test.ts that loads each doc-set mcp-corpus, builds the index, runs all queries for that doc-set from query-suite.json, and measures: precision@1 (is the top result the expected one?), precision@3 (is the expected result in top 3?), MRR (Mean Reciprocal Rank). Report per-doc-set and aggregate metrics. Write results to test-output/search-quality-report.json. Assert aggregate precision@1 >= 0.5 as initial baseline (to be raised as search improves). This replaces the synthetic search-golden.test.ts with real-data evaluation.",
      "acceptance_criteria": [
        "Test loads query-suite.json and groups queries by doc_name — verify: test setup logs query count per doc",
        "For each doc-set: builds MarkdownParser index on mcp-corpus, runs all queries — verify: test iterates all 10 doc-sets",
        "Measures precision@1: percentage of queries where top result matches expected_top_title — verify: metric calculation in test",
        "Measures precision@3: percentage where expected result is in top 3 — verify: metric calculation",
        "Measures MRR (Mean Reciprocal Rank) per doc-set — verify: MRR formula implemented correctly",
        "Writes search-quality-report.json with per-doc and aggregate metrics — verify: file written and valid JSON",
        "Asserts aggregate precision@1 >= 0.5 — verify: assertion in test (baseline, will be raised)",
        "Asserts aggregate precision@3 >= 0.7 — verify: assertion in test"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-RealData",
      "feature": "Large corpus performance test with onoffice and tailwind",
      "description": "Create mcp-server/src/__tests__/large-corpus-perf.test.ts that measures index build time and search latency against the two largest doc-sets: onoffice (645 files) and tailwind (560 files). Uses the full scraped output from %APPDATA%/AnyDocsMCP/docs (not the small mcp-corpus subset). If the full docs are not available in CI, the test gracefully skips with a message. Measures: index build time (must be <5s for 600+ file corpus), search p95 latency (must be <100ms), memory usage delta during indexing. Results written to test-output/large-corpus-perf.json.",
      "acceptance_criteria": [
        "Test checks if large corpus exists at expected path, skips gracefully if not — verify: test.skipIf logic",
        "Index build time measured for onoffice (645 files) — verify: timing assertion <5s",
        "Index build time measured for tailwind (560 files) — verify: timing assertion <5s",
        "Search p95 latency measured across 50 queries on large corpus — verify: p95 < 100ms assertion",
        "Reports section count, file count, index build time, search p50/p95 per corpus — verify: output JSON structure",
        "Writes large-corpus-perf.json with all metrics — verify: file is valid JSON",
        "Test passes in both CI (skip) and local (run) modes — verify: no hard failures when corpus absent"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-RealData",
      "feature": "Cross-file search quality evaluation",
      "description": "Add 20 cross-file queries to query-suite.json that test search across multiple files within a doc-set. These queries should return results from at least 2 different files. For example, searching 'authentication' in fastapi should return results from both tutorial-security.md and advanced-security.md. Extend the search benchmark to measure cross-file recall: what percentage of expected files appear in the top 5 results. This tests the search engine's ability to surface relevant content distributed across the documentation.",
      "acceptance_criteria": [
        "20 additional cross-file queries added to query-suite.json — verify: queries with expected_files (plural) field",
        "Each cross-file query has expected_files array with 2+ file names — verify: schema validation",
        "Cross-file queries span at least 5 different doc-sets — verify: group-by count",
        "Search benchmark measures cross-file recall: pct of expected files found in top 5 results — verify: metric in report",
        "Aggregate cross-file recall >= 0.4 as baseline — verify: assertion in test",
        "Results included in search-quality-report.json under cross_file_metrics key — verify: JSON structure"
      ],
      "passes": false
    },
    {
      "group": "Search-Relevance-RealData",
      "feature": "Code-specific search evaluation against real code examples",
      "description": "Add 15 code-specific queries to query-suite.json that target specific code patterns in the documentation. Examples: 'React useState hook example', 'FastAPI path parameters', 'Tailwind flex container', 'Kubernetes pod spec YAML', 'Django model definition'. Each query has expected_code_language and expected_code_snippet (a substring that must appear in a code block of the top result). Extend the search benchmark to measure code-search precision: does the top result contain a code block matching the expected language and snippet? This directly tests the code block boosting logic from v1.",
      "acceptance_criteria": [
        "15 code-specific queries added to query-suite.json with expected_code_language and expected_code_snippet fields — verify: schema validation",
        "Queries cover at least 5 different programming languages (JS, Python, YAML, TypeScript, bash) — verify: language diversity check",
        "Search benchmark measures code-search precision: top result has matching code block — verify: metric in report",
        "Code match checks both language and snippet substring — verify: test logic inspection",
        "Aggregate code-search precision >= 0.5 as baseline — verify: assertion",
        "Results included in search-quality-report.json under code_search_metrics key — verify: JSON structure"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Testing",
      "feature": "Full pipeline smoke test per reference doc-set via MockServer",
      "description": "Create scraper/tests/test_pipeline_e2e.py with parametrized tests that run the complete Scrape->Clean->Write pipeline for each reference doc-set using the FixtureHTTPServer. For each doc-set: start MockServer with captured fixtures, configure ScraperEngine with rewritten URLs, run scrape_all(), verify output files are created with non-empty content. This is the first true end-to-end test that exercises the real pipeline without network access. Start with 3 doc-sets (react, fastapi, hyperapp-github) that have captured fixtures available.",
      "acceptance_criteria": [
        "Test is parametrized over doc-sets that have captured fixtures — verify: pytest parametrize decorator",
        "MockServer starts and serves captured HTML responses — verify: test setup starts server",
        "ScraperEngine is configured with rewritten base URL pointing to MockServer — verify: config uses mock_server.get_rewritten_url()",
        "scrape_all() completes without exceptions — verify: no pytest errors",
        "At least 1 output .md file is created per doc-set — verify: output file count > 0",
        "Output .md files contain real content (>500 chars each) — verify: file size assertion",
        "Test runs in <30s per doc-set — verify: pytest timeout",
        "At least 3 doc-sets tested (react, fastapi, hyperapp-github) — verify: parametrize list length"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Testing",
      "feature": "Golden output snapshot comparison with diff reporting",
      "description": "Extend the pipeline E2E tests with golden output comparison. After scraping via MockServer, compare the produced markdown output against the golden snapshots in tests/fixtures/real-world/{doc-name}/golden/. Use a structured diff that ignores whitespace variations and reports: lines_added, lines_removed, sections_changed, encoding_changes, artifact_changes. The test PASSES if the output is equal to or BETTER than the golden (fewer artifacts, fewer encoding errors). It FAILS only if the output is WORSE (more artifacts or new encoding errors introduced). This enables fearless refactoring.",
      "acceptance_criteria": [
        "compare_with_golden(produced, golden) returns QualityDiff with metrics — verify: unit test with known diff",
        "QualityDiff includes: lines_changed, encoding_errors_delta, artifacts_delta, quality_score_delta — verify: field assertions",
        "Test PASSES when produced output has fewer encoding errors than golden — verify: test with improved output",
        "Test PASSES when produced output is identical to golden — verify: test with unchanged output",
        "Test FAILS when produced output has MORE encoding errors than golden — verify: test with degraded output",
        "Diff report is written to test-output/golden-diff-{doc-name}.json — verify: file existence",
        "Human-readable summary logged showing what improved/degraded — verify: test output inspection"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Testing",
      "feature": "MCP tool integration tests against real doc corpora",
      "description": "Create mcp-server/src/__tests__/realworld-tools.test.ts that tests all MCP tools (search, get_overview, get_file_toc, get_section, find_code_examples) against the real mcp-corpus fixtures. For each doc-set: build index, call each tool, verify the response is well-formed and contains expected content from the real docs. This replaces the synthetic e2e-tools.test.ts with real-data validation. Specifically tests: overview contains real section titles, TOC has proper heading hierarchy, search returns relevant sections, code examples have correct language tags.",
      "acceptance_criteria": [
        "Test iterates over all doc-sets with mcp-corpus fixtures — verify: parametrized over fixture dirs",
        "getOverview() returns overview containing real file names from the corpus — verify: string matching",
        "getFileToc() returns non-empty TOC for each file in the corpus — verify: length > 0",
        "search() for a known term returns at least 1 result — verify: results.length > 0",
        "Code blocks in search results have non-empty language tags — verify: codeBlocks[0].language check",
        "getSection() with a valid section ID returns content — verify: section.content.length > 0",
        "All tests pass across at least 5 doc-set corpora — verify: no test failures"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Testing",
      "feature": "Quality dashboard JSON report after each test run",
      "description": "Create a quality dashboard that aggregates all test results into a single comprehensive JSON report: test-output/quality-dashboard.json. Includes: per-doc-set metrics (encoding_errors, artifacts, quality_score, search_precision, index_build_time), aggregate metrics (total_encoding_errors, avg_quality_score, overall_precision), trend data (current vs golden baseline from quality_baseline.json showing improvement/degradation). A simple Node.js script renders this JSON as a markdown table to stdout for quick review. The dashboard is the single source of truth for 'is the system getting better?'.",
      "acceptance_criteria": [
        "quality-dashboard.json is generated after vitest run — verify: file exists after npm test",
        "Contains per-doc-set section with encoding_errors, artifacts, quality_score, search_precision — verify: JSON schema validation",
        "Contains aggregate section with totals and averages — verify: field assertions",
        "Contains trend section comparing current metrics to quality_baseline.json — verify: delta fields present",
        "Trend shows improvement_pct per metric — verify: calculation test",
        "render-dashboard.js script outputs markdown table to stdout — verify: script runs without error",
        "Dashboard correctly reflects test results (not hardcoded) — verify: changing a metric changes dashboard output"
      ],
      "passes": false
    },
    {
      "group": "E2E-Pipeline-Testing",
      "feature": "CI pipeline update with real-world test gates",
      "description": "Update .github/workflows/test.yml to include the real-world test suite as an additional CI job. The job runs after the basic unit tests pass. It requires the captured fixtures to be present in the repo (tests/fixtures/real-world/). The job runs: (1) Python encoding regression tests, (2) Python pipeline E2E tests with MockServer, (3) TypeScript real-world search benchmark, (4) Quality dashboard generation. The job fails if: any encoding regression test fails, search precision@1 drops below baseline, or pipeline E2E tests produce worse output than golden. Large corpus perf tests are skipped in CI (local-only).",
      "acceptance_criteria": [
        "New CI job 'real-world-tests' defined in test.yml — verify: YAML inspection",
        "Job depends on basic unit test jobs passing first — verify: needs: field in YAML",
        "Runs Python encoding regression tests — verify: step with pytest test_encoding_realworld.py",
        "Runs TypeScript real-world search benchmark — verify: step with npx vitest run realworld-search",
        "Generates quality dashboard as CI artifact — verify: upload-artifact step for test-output/",
        "Large corpus perf tests are skipped in CI — verify: test.skipIf or CI env check",
        "Pipeline E2E tests run with MockServer — verify: step with pytest test_pipeline_e2e.py"
      ],
      "passes": false
    }
  ]
}