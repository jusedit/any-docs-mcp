"""
Multi-mode URL discovery for documentation scraping.

Architecture (3-phase):
1. SCOUT CRAWL - Visit ~10 pages from start URL, collect ALL links (no filtering)
2. LLM SCOPE  - Send link list to LLM → get regex include/exclude rules
3. FULL DISCOVERY - Sitemap + Navigation + Crawl, filtered by LLM rules

Falls back to heuristic-based scope when LLM is unavailable.
"""
import re
import json
import time
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Set, Tuple
from urllib.parse import urlparse, urljoin
from collections import deque, defaultdict
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import os

from sitemap_parser import SitemapParser
from github_discovery import GitHubDiscovery
from webdriver_discovery import WebDriverDiscovery, SELENIUM_AVAILABLE


@dataclass
class ScopeRules:
    """Dynamic URL scope rules — replaces hardcoded path prefix matching.
    
    Generated by LLM analysis of scout-crawled links, or from path prefixes as fallback.
    """
    include_patterns: List[re.Pattern] = field(default_factory=list)
    exclude_patterns: List[re.Pattern] = field(default_factory=list)
    base_url: str = ""
    description: str = ""
    
    def url_matches(self, url: str) -> bool:
        """Check if URL should be included based on scope rules."""
        if not url.startswith(self.base_url):
            return False
        
        # Exclude patterns take priority
        for pattern in self.exclude_patterns:
            if pattern.search(url):
                return False
        
        # Must match at least one include pattern (if any defined)
        if self.include_patterns:
            return any(p.search(url) for p in self.include_patterns)
        
        # No include patterns = include everything on same domain
        return True
    
    @classmethod
    def from_path_prefixes(cls, base_url: str, scopes: List[str]) -> 'ScopeRules':
        """Create ScopeRules from old-style path prefix scopes (heuristic fallback)."""
        includes = []
        for scope in scopes:
            if scope == '/':
                includes.append(re.compile(re.escape(base_url) + '/.*'))
            else:
                escaped = re.escape(base_url + scope.rstrip('/'))
                includes.append(re.compile(escaped + '(/.*)?$'))
        
        return cls(
            include_patterns=includes,
            exclude_patterns=[],
            base_url=base_url,
            description=f"Heuristic fallback: path prefixes {scopes}"
        )
    
    @classmethod
    def from_llm_response(cls, base_url: str, include_strs: List[str], 
                          exclude_strs: List[str], description: str = "") -> 'ScopeRules':
        """Create ScopeRules from LLM-generated regex strings."""
        includes = []
        excludes = []
        
        for pattern_str in include_strs:
            try:
                includes.append(re.compile(pattern_str, re.IGNORECASE))
            except re.error as e:
                print(f"    Warning: invalid include regex '{pattern_str}': {e}")
        
        for pattern_str in exclude_strs:
            try:
                excludes.append(re.compile(pattern_str, re.IGNORECASE))
            except re.error as e:
                print(f"    Warning: invalid exclude regex '{pattern_str}': {e}")
        
        return cls(
            include_patterns=includes,
            exclude_patterns=excludes,
            base_url=base_url,
            description=description
        )


class URLDiscovery:
    """Multi-mode URL discovery for documentation sites."""
    
    MODES = ['github', 'sitemap', 'navigation', 'crawl']
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key
        ) if self.api_key else None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; AnyDocsMCP/1.0)'
        })
        self.github_discovery = GitHubDiscovery()
    
    def discover_urls(self, start_url: str, max_pages: int = 500, locale_filter: Optional[str] = None, use_webdriver: bool = False) -> Dict:
        """
        Discover documentation URLs using a 3-phase approach:
        
        Phase 1: Scout crawl — visit ~10 pages, collect ALL links (no filtering)
        Phase 2: LLM scope  — send links to LLM, get regex include/exclude rules
        Phase 3: Full discovery — sitemap + navigation + crawl, filtered by LLM rules
        
        Falls back to heuristic-based scope when LLM is unavailable.
        """
        parsed = urlparse(start_url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        # Detect version and locale from URL
        version = self._detect_version(start_url)
        if locale_filter is None:
            locale_filter = self._detect_locale(start_url)
        
        print(f"  URL Discovery for: {start_url}")
        print(f"  Detected version: {version or 'none'}")
        print(f"  Locale filter: {locale_filter or 'none'}")
        
        # 0. GitHub repos use exclusive mode (no scout needed)
        if self.github_discovery.is_github_repo(start_url):
            print(f"  Detected GitHub repository")
            github_files = self.github_discovery.discover_markdown_files(start_url, max_pages)
            github_files = self._apply_locale_filter(github_files, locale_filter)
            if github_files:
                return {
                    'mode': 'github',
                    'urls': github_files,
                    'version': version,
                    'scope_rules': None,
                    'locale': locale_filter,
                    'stats': {'raw': len(github_files), 'filtered': len(github_files)}
                }
            print(f"  GitHub discovery found no files, falling back")
        
        # --- PHASE 1: Scout crawl ---
        scout_html, scout_links = self._scout_crawl(start_url, max_pages=10)
        print(f"  Scout crawl: found {len(scout_links)} unique links on {min(10, len(scout_links))} pages")
        
        # --- PHASE 2: LLM scope determination ---
        if self.client:
            scope_rules = self._llm_determine_scope(start_url, base_url, scout_html, scout_links)
        else:
            # Fallback: heuristic-based scope from path prefixes
            old_scopes = self._determine_scope(start_url)
            scope_rules = ScopeRules.from_path_prefixes(base_url, old_scopes)
        
        print(f"  Scope: {scope_rules.description}")
        
        # --- PHASE 3: Full discovery using scope rules ---
        all_urls = {}
        mode_stats = {}
        primary_mode = None
        
        # 3a. Try sitemap
        sitemap_urls = self._try_sitemap(base_url, scope_rules, max_pages, locale_filter)
        if sitemap_urls:
            mode_stats['sitemap'] = len(sitemap_urls)
            for u in sitemap_urls:
                if u['url'] not in all_urls:
                    all_urls[u['url']] = {**u, 'source': u.get('source', 'sitemap')}
            if len(sitemap_urls) >= 10:
                primary_mode = 'sitemap'
            print(f"  Sitemap: {len(sitemap_urls)} URLs")
        
        # 3b. Try navigation (always, even if sitemap succeeded)
        nav_urls = self._try_navigation(start_url, scope_rules, locale_filter, use_webdriver=use_webdriver)
        if nav_urls:
            mode_stats['navigation'] = len(nav_urls)
            new_from_nav = 0
            for u in nav_urls:
                if u['url'] not in all_urls:
                    all_urls[u['url']] = {**u, 'source': u.get('source', 'navigation')}
                    new_from_nav += 1
            if not primary_mode and len(nav_urls) >= 5:
                primary_mode = 'navigation'
            print(f"  Navigation: {len(nav_urls)} URLs ({new_from_nav} new)")
        
        # 3c. Supplement with crawl if combined results are still sparse
        combined_count = len(all_urls)
        if combined_count < max(10, max_pages // 10):
            crawl_budget = max_pages - combined_count
            crawl_urls = self._crawl_links(start_url, scope_rules, crawl_budget, locale_filter)
            if crawl_urls:
                mode_stats['crawl'] = len(crawl_urls)
                new_from_crawl = 0
                for u in crawl_urls:
                    if u['url'] not in all_urls:
                        all_urls[u['url']] = {**u, 'source': 'crawl'}
                        new_from_crawl += 1
                if not primary_mode:
                    primary_mode = 'crawl'
                print(f"  Crawl: {len(crawl_urls)} URLs ({new_from_crawl} new)")
        
        # Build final result
        final_urls = list(all_urls.values())[:max_pages]
        mode = primary_mode or 'crawl'
        
        contributing_modes = [m for m, c in mode_stats.items() if c > 0]
        if len(contributing_modes) > 1:
            mode = f"hybrid({'+'.join(contributing_modes)})"
        
        result = {
            'mode': mode,
            'urls': final_urls,
            'version': version,
            'scope_rules': scope_rules,
            'locale': locale_filter,
            'stats': {
                'raw': sum(mode_stats.values()),
                'filtered': len(final_urls),
                'by_mode': mode_stats
            }
        }
        print(f"  Mode: {mode.upper()} ({len(final_urls)} URLs total)")
        return result
    
    def _scout_crawl(self, start_url: str, max_pages: int = 10) -> Tuple[str, Set[str]]:
        """Phase 1: Quick BFS crawl to discover site structure. No scope filtering.
        
        Visits start_url + up to max_pages internal pages, collecting ALL links.
        Returns (start_page_html, set_of_all_unique_links_found).
        """
        parsed_start = urlparse(start_url)
        base_url = f"{parsed_start.scheme}://{parsed_start.netloc}"
        
        all_links: Set[str] = set()
        visited: Set[str] = set()
        queue = deque([start_url])
        start_html = ""
        pages_visited = 0
        
        while queue and pages_visited < max_pages:
            url = queue.popleft()
            
            # Normalize for dedup
            norm = self._normalize_url(url, url, base_url)
            if not norm or norm in visited:
                continue
            visited.add(norm)
            
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code != 200:
                    continue
                if 'text/html' not in response.headers.get('content-type', ''):
                    continue
            except Exception:
                continue
            
            html = response.text
            if pages_visited == 0:
                start_html = html
            pages_visited += 1
            
            # Extract ALL links from this page
            soup = BeautifulSoup(html, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a.get('href', '').split('#')[0].strip()
                if not href or href.startswith(('javascript:', 'mailto:', 'tel:')):
                    continue
                
                full_url = urljoin(url, href)
                full_parsed = urlparse(full_url)
                
                # Only collect same-domain links
                if full_parsed.netloc == parsed_start.netloc:
                    # Strip query params and fragment for cleaner URL
                    clean = f"{full_parsed.scheme}://{full_parsed.netloc}{full_parsed.path}"
                    all_links.add(clean)
                    
                    # Add to BFS queue if not yet visited (for exploration)
                    if clean not in visited and self._is_navigable(clean):
                        queue.append(clean)
        
        return start_html, all_links
    
    def _is_navigable(self, url: str) -> bool:
        """Check if URL is worth visiting during scout crawl (skip binary files)."""
        path = urlparse(url).path.lower()
        skip_ext = {'.pdf', '.zip', '.tar', '.gz', '.png', '.jpg', '.gif', '.svg',
                    '.css', '.js', '.woff', '.woff2', '.ttf', '.ico', '.xml', '.json'}
        return not any(path.endswith(ext) for ext in skip_ext)
    
    def _llm_determine_scope(self, start_url: str, base_url: str, 
                              start_html: str, scout_links: Set[str]) -> ScopeRules:
        """Phase 2: Ask LLM to analyze scout-crawled links and return scope rules.
        
        Groups links by path prefix for efficient context, sends to LLM,
        gets back include/exclude regex patterns.
        """
        # Group links by first path segment for compact representation
        groups: Dict[str, List[str]] = defaultdict(list)
        for link in sorted(scout_links):
            parsed = urlparse(link)
            segments = parsed.path.strip('/').split('/')
            group_key = f"/{segments[0]}/" if segments and segments[0] else "/"
            groups[group_key].append(parsed.path)
        
        # Build compact link summary for prompt
        link_summary_parts = []
        for group_key in sorted(groups.keys()):
            paths = groups[group_key]
            link_summary_parts.append(f"\n{group_key} ({len(paths)} URLs)")
            # Show up to 5 examples per group
            for p in paths[:5]:
                link_summary_parts.append(f"  {p}")
            if len(paths) > 5:
                link_summary_parts.append(f"  ... and {len(paths) - 5} more")
        
        link_summary = "\n".join(link_summary_parts)
        
        # Sanitize start page HTML (remove scripts/styles, limit size)
        soup = BeautifulSoup(start_html, 'html.parser')
        for tag in soup(['script', 'style', 'noscript', 'iframe']):
            tag.decompose()
        
        # Extract page title and meta description for context
        title = soup.find('title')
        title_text = title.get_text(strip=True) if title else "Unknown"
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        desc_text = meta_desc.get('content', '') if meta_desc else ""
        
        prompt = f"""You are analyzing a documentation website to determine which URLs are documentation pages vs non-documentation pages (blog, marketing, auth, etc).

**Target documentation site:** {start_url}
**Page title:** {title_text}
**Description:** {desc_text}
**Base URL:** {base_url}
**Total unique links found:** {len(scout_links)}

**Links grouped by path prefix:**
{link_summary}

Based on this analysis, provide regex patterns to INCLUDE documentation URLs and EXCLUDE non-documentation URLs.

**Rules:**
1. include_patterns: Python regex patterns that match documentation page URLs. Be generous — include API references, guides, tutorials, getting-started pages, etc.
2. exclude_patterns: Python regex patterns that match NON-documentation URLs (blog, news, pricing, login, social, downloads, other languages if a specific language was in the start URL, etc.)
3. Patterns are matched against the FULL URL (e.g., "https://example.com/docs/intro")
4. Use simple, robust patterns. Prefer path-based patterns like "/docs/" over complex regex
5. If the start URL contains a language prefix like /en/, exclude other language prefixes
6. If the start URL contains a version like /stable/ or /3.13/, only include that version

**Examples of good patterns:**
- Include: ["/docs/", "/api/", "/reference/"]  → matches https://example.com/docs/anything
- Exclude: ["/blog/", "/news/", "/(fr|de|ja|zh)/"]  → blocks non-doc URLs

Return ONLY valid JSON:
{{"include_patterns": ["/pattern1/", "/pattern2/"], "exclude_patterns": ["/blog/", "/other/"], "reasoning": "brief explanation"}}"""
        
        try:
            response = self.client.chat.completions.create(
                model="anthropic/claude-haiku-4.5",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # Clean JSON wrapper
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.startswith("```"):
                result_text = result_text[3:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            result_text = result_text.strip()
            
            filters = json.loads(result_text)
            reasoning = filters.get('reasoning', '')
            print(f"  LLM Scope: {reasoning[:80]}")
            
            scope_rules = ScopeRules.from_llm_response(
                base_url=base_url,
                include_strs=filters.get('include_patterns', []),
                exclude_strs=filters.get('exclude_patterns', []),
                description=f"LLM: {reasoning[:100]}"
            )
            
            # Sanity check: at least some scout links should match (skip if no scout links)
            if scout_links:
                matching = sum(1 for link in scout_links if scope_rules.url_matches(link))
                if matching < 3:
                    print(f"  Warning: LLM scope too restrictive ({matching} matches). Using heuristic fallback.")
                    old_scopes = self._determine_scope(start_url)
                    return ScopeRules.from_path_prefixes(base_url, old_scopes)
                print(f"  LLM scope matches {matching}/{len(scout_links)} scout links")
            else:
                print(f"  LLM scope (no scout links to validate against)")
            return scope_rules
            
        except Exception as e:
            print(f"  LLM scope determination failed: {e}. Using heuristic fallback.")
            old_scopes = self._determine_scope(start_url)
            return ScopeRules.from_path_prefixes(base_url, old_scopes)
    
    def _detect_locale(self, url: str) -> Optional[str]:
        """Detect language/locale from URL path.
        
        Examples:
            /en/stable/ -> 'en'
            /de/5.0/ -> 'de'
            /docs/latest/ -> None (no locale)
            /el/ (Greek Django) -> 'el'
        """
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return None
        
        # First path segment is often the locale
        # Pattern: /{locale}/... where locale is 2-3 letters
        first_segment = path.split('/')[0]
        
        # Check if first segment looks like a locale code
        # Valid: 'en', 'de', 'fr', 'ja', 'el', 'zh-hans', etc.
        # Not valid: 'docs', 'stable', 'v1', '2024'
        if re.match(r'^[a-z]{2}(-[a-z]+)?$', first_segment):
            # Exclude common non-locale path segments
            non_locales = {'docs', 'api', 'blog', 'about', 'help', 'search', 'v1', 'v2'}
            if first_segment not in non_locales:
                return first_segment
        
        return None
    
    # Known non-locale 2-letter path segments (would false-positive as locale codes)
    _NON_LOCALE_SEGMENTS = frozenset({
        'js', 'cs', 'go', 'py', 'ts', 'ui', 'ci', 'cd', 'qa', 'db', 'io', 'os',
        'ai', 'ml', 'dl', 'v1', 'v2', 'v3', 'v4', 'v5',
    })
    
    def _apply_locale_filter(self, urls: List[Dict], locale_filter: str) -> List[Dict]:
        """Filter URLs to only include those matching the target locale.
        
        Uses generic regex detection: any /{xx}/ path segment matching ISO 639-1
        pattern (2 lowercase letters) is treated as a locale, unless it's a known
        non-locale segment. URLs with a detected locale different from locale_filter
        are excluded. URLs without any detected locale are kept.
        """
        if not locale_filter:
            return urls
        
        target = locale_filter.lower()
        filtered = []
        
        for u in urls:
            parsed = urlparse(u['url'])
            path = parsed.path.lower()
            
            # Find all potential locale segments: /{xx}/ at any position
            locale_matches = re.findall(r'/([a-z]{2})/', path)
            
            # Determine detected locale (first match that isn't a known non-locale)
            detected_locale = None
            for match in locale_matches:
                if match not in self._NON_LOCALE_SEGMENTS:
                    detected_locale = match
                    break
            
            # Keep URL if no locale detected or if it matches target
            if detected_locale is None or detected_locale == target:
                filtered.append(u)
        
        if len(filtered) < len(urls):
            print(f"    Locale filter ({locale_filter}): {len(urls)} -> {len(filtered)} URLs")
        
        return filtered
    
    def _detect_version(self, url: str) -> Optional[str]:
        """Detect documentation version from URL."""
        parsed = urlparse(url)
        path = parsed.path
        
        # Common version patterns
        patterns = [
            r'/v?(\d+\.\d+(?:\.\d+)?)',  # /3.13/, /v2.0/, /1.2.3/
            r'/(stable|latest|current|dev|main|master)/',  # /stable/, /latest/
            r'/en/(\d+\.\d+)',  # Django style: /en/5.0/
        ]
        
        for pattern in patterns:
            match = re.search(pattern, path)
            if match:
                return match.group(1)
        
        return None
    
    def _determine_scope(self, url: str) -> List[str]:
        """Determine the URL scope (path prefixes) for filtering.
        
        Returns a list of scope prefixes. For root URLs, analyzes the page
        to find documentation paths. For specific paths, returns the directory prefix.
        """
        parsed = urlparse(url)
        path = parsed.path.rstrip('/')
        
        # For root URLs, analyze the page to find doc paths
        if not path or path == '/':
            doc_paths = self._analyze_documentation_paths(url)
            if doc_paths:
                return doc_paths
            return ['/']
        
        # Strip filename from file URLs (e.g., /docs/home.html -> /docs/)
        last_segment = path.rsplit('/', 1)[-1]
        if '.' in last_segment:
            path = path.rsplit('/', 1)[0]
        
        # Ensure the scope is a directory prefix
        if not path or path == '/':
            return ['/']
        return [path + '/']
    
    def _analyze_documentation_paths(self, start_url: str) -> List[str]:
        """Analyze page to find documentation path prefixes.
        
        Fetches the start page, extracts links from nav elements,
        groups by path prefix, and scores each group.
        Returns top-scoring documentation path prefixes.
        """
        try:
            response = self.session.get(start_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"  Doc path analysis failed: {e}")
            return []
        
        parsed_start = urlparse(start_url)
        base_url = f"{parsed_start.scheme}://{parsed_start.netloc}"
        
        # Find all links in navigation elements
        nav_selectors = [
            'nav', 'aside', '.sidebar', '#sidebar', '.toc', '#toc',
            '.navigation', '.nav-menu', '.docs-nav', '.doc-sidebar',
            "[role='navigation']", '.menu', '#menu', 'header', '.header'
        ]
        
        path_scores = {}
        
        for selector in nav_selectors:
            try:
                for nav in soup.select(selector):
                    for a in nav.find_all('a', href=True):
                        href = a.get('href', '')
                        full_url = self._normalize_url(href, start_url, base_url)
                        if not full_url:
                            continue
                        
                        parsed = urlparse(full_url)
                        path = parsed.path.strip('/')
                        if not path:
                            continue
                        
                        # Get first path segment
                        first_segment = path.split('/')[0]
                        if not first_segment:
                            continue
                        
                        # Score the path
                        score = 0
                        
                        # Links in nav elements get higher weight
                        score += 2
                        
                        # Doc-like paths get bonus
                        doc_patterns = ['docs', 'documentation', 'guide', 'reference', 
                                       'api', 'learn', 'tutorial', 'handbook', 'manual']
                        if any(p in first_segment.lower() for p in doc_patterns):
                            score += 3
                        
                        # Non-doc paths get penalty
                        non_doc_patterns = ['blog', 'news', 'community', 'forum', 
                                           'about', 'contact', 'pricing', 'enterprise']
                        if any(p in first_segment.lower() for p in non_doc_patterns):
                            score -= 5
                        
                        # Track score
                        prefix = '/' + first_segment + '/'
                        if prefix not in path_scores:
                            path_scores[prefix] = 0
                        path_scores[prefix] += score
            except Exception:
                continue
        
        # Return paths with positive scores, sorted by score
        positive_paths = [(p, s) for p, s in path_scores.items() if s > 0]
        positive_paths.sort(key=lambda x: x[1], reverse=True)
        
        if positive_paths:
            # Return top paths (max 5)
            return [p for p, _ in positive_paths[:5]]
        
        return []
    
    def _try_sitemap(self, base_url: str, scope_rules: ScopeRules, max_pages: int, locale_filter: Optional[str] = None) -> List[Dict]:
        """Try to get URLs from sitemap, filtered by ScopeRules."""
        parser = SitemapParser(base_url)
        
        if not parser.has_sitemap():
            return []
        
        urls = parser.parse_sitemap()
        if not urls:
            return []
        
        # Filter by scope rules
        urls = [u for u in urls if scope_rules.url_matches(u['url'])]
        
        # Apply locale filter
        urls = self._apply_locale_filter(urls, locale_filter)
        
        # Cap at reasonable limit
        if len(urls) > max_pages * 2:
            urls = urls[:max_pages * 2]
        
        return urls
    
    def _score_and_filter_sitemap_urls(self, urls: List[Dict], scopes: List[str], base_url: str, locale_filter: Optional[str] = None) -> List[Dict]:
        """Group and score sitemap URLs to filter out non-documentation pages."""
        from collections import defaultdict
        
        # Group URLs by first 2 path segments
        groups = defaultdict(list)
        for u in urls:
            parsed = urlparse(u['url'])
            path_segments = parsed.path.strip('/').split('/')
            
            # Create group key from first 2 segments (or 1 if only 1 exists)
            if len(path_segments) >= 2:
                group_key = f"/{path_segments[0]}/{path_segments[1]}/"
            elif len(path_segments) == 1:
                group_key = f"/{path_segments[0]}/"
            else:
                group_key = "/"
            
            groups[group_key].append(u)
        
        # Score each group
        doc_patterns = ['docs', 'api', 'guide', 'reference', 'tutorial', 'learn', 'manual', 'handbook']
        non_doc_patterns = ['blog', 'news', 'community', 'forum', 'about', 'contact', 'pricing']
        
        group_scores = {}
        for group_key, group_urls in groups.items():
            score = 0
            
            # Doc-like paths get higher scores
            if any(pattern in group_key.lower() for pattern in doc_patterns):
                score += 10
            
            # Non-doc paths get negative scores
            if any(pattern in group_key.lower() for pattern in non_doc_patterns):
                score -= 10
            
            # Translation patterns: penalize locales that don't match preferred language
            preferred_lang = locale_filter.lower() if locale_filter else 'en'
            translation_pattern = re.match(r'^/([a-z]{2})/', group_key)
            if translation_pattern:
                lang = translation_pattern.group(1)
                if lang != preferred_lang:
                    score -= 5
            
            # Groups matching current scopes get bonus
            for scope in scopes:
                if scope in group_key:
                    score += 3
            
            # Size bonus/penalty - very large groups might be problematic
            if len(group_urls) > 100:
                score -= 2  # Slight penalty for very large groups
            
            group_scores[group_key] = score
        
        # Keep only positive-scoring groups
        positive_groups = [gk for gk, score in group_scores.items() if score > 0]
        
        # If no positive groups, keep all (fallback)
        if not positive_groups:
            return urls
        
        # Build URL → group_key lookup for O(1) score access
        url_to_group = {}
        for group_key in positive_groups:
            for u in groups[group_key]:
                url_to_group[u['url']] = group_key
        
        # Combine URLs from positive-scoring groups, sorted by score
        filtered_urls = []
        for group_key in positive_groups:
            filtered_urls.extend(groups[group_key])
        
        filtered_urls.sort(
            key=lambda u: group_scores.get(url_to_group.get(u['url'], ''), 0),
            reverse=True
        )
        
        print(f"  Sitemap: grouped {len(urls)} URLs into {len(groups)} groups, kept {len(filtered_urls)} from {len(positive_groups)} positive-scoring groups")
        
        return filtered_urls
    
    def _try_navigation(self, start_url: str, scope_rules: ScopeRules, locale_filter: Optional[str] = None, max_level1_pages: int = 20, use_webdriver: bool = False) -> List[Dict]:
        """Try to extract URLs from navigation menus and SPA data.
        
        Implements 2-level recursive extraction:
        Level 0: Extract nav links from start page
        Level 1: For section pages, fetch and extract THEIR nav links
        """
        try:
            response = self.session.get(start_url, timeout=30)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
        except Exception as e:
            print(f"  Navigation extraction failed: {e}")
            return []
        
        parsed_start = urlparse(start_url)
        base_url = f"{parsed_start.scheme}://{parsed_start.netloc}"
        
        urls = []
        seen = set()
        
        # 1. Try standard navigation selectors (Level 0)
        nav_selectors = [
            'nav', 'aside', '.sidebar', '#sidebar', '.toc', '#toc',
            '.navigation', '.nav-menu', '.docs-nav', '.doc-sidebar',
            "[role='navigation']", '.menu', '#menu'
        ]
        
        for selector in nav_selectors:
            try:
                for nav in soup.select(selector):
                    for a in nav.find_all('a', href=True):
                        href = a.get('href', '')
                        full_url = self._normalize_url(href, start_url, base_url)
                        
                        if full_url and full_url not in seen:
                            if scope_rules.url_matches(full_url):
                                seen.add(full_url)
                                urls.append({
                                    'url': full_url,
                                    'title': a.get_text(strip=True)[:100]
                                })
            except Exception:
                continue
        
        # 2. Try extracting from SPA data (Next.js, Docusaurus)
        if len(urls) < 5:
            spa_urls = self._extract_spa_navigation(html_content, start_url, base_url, scope_rules)
            for url_info in spa_urls:
                if url_info['url'] not in seen:
                    seen.add(url_info['url'])
                    urls.append(url_info)
        
        # 3. Fallback: extract from content area if still < 5 URLs
        if len(urls) < 5:
            content_urls = self._extract_content_area_links(soup, start_url, base_url, scope_rules, seen)
            urls.extend(content_urls)
        
        # 4. Level 1: Recursive extraction from section pages
        section_urls = [u for u in urls if self._is_section_page(u['url'])]
        section_urls = section_urls[:max_level1_pages]
        
        if section_urls:
            print(f"  Nav Level 1: scanning {len(section_urls)} section pages...")
            for section_url_info in section_urls:
                try:
                    time.sleep(0.5)
                    response = self.session.get(section_url_info['url'], timeout=15)
                    if response.status_code != 200:
                        continue
                    
                    section_soup = BeautifulSoup(response.text, 'html.parser')
                    
                    for selector in nav_selectors:
                        try:
                            for nav in section_soup.select(selector):
                                for a in nav.find_all('a', href=True):
                                    href = a.get('href', '')
                                    full_url = self._normalize_url(href, section_url_info['url'], base_url)
                                    
                                    if full_url and full_url not in seen:
                                        if scope_rules.url_matches(full_url):
                                            seen.add(full_url)
                                            urls.append({
                                                'url': full_url,
                                                'title': a.get_text(strip=True)[:100]
                                            })
                        except Exception:
                            continue
                            
                except Exception:
                    continue
        
        # 5. WebDriver escalation
        script_tags = len(soup.find_all('script'))
        should_escalate = (len(urls) < 10 and script_tags > 3) or use_webdriver
        if should_escalate and SELENIUM_AVAILABLE:
            print(f"  WebDriver escalation: {len(urls)} URLs found, {script_tags} script tags detected")
            try:
                webdriver_discovery = WebDriverDiscovery()
                try:
                    wd_result = webdriver_discovery.discover_urls(start_url, max_pages=50)
                    wd_url_list = wd_result.get('urls', []) if isinstance(wd_result, dict) else []
                    
                    new_count = 0
                    for url_info in wd_url_list:
                        if url_info['url'] not in seen:
                            if scope_rules.url_matches(url_info['url']):
                                seen.add(url_info['url'])
                                urls.append({
                                    'url': url_info['url'],
                                    'title': url_info.get('title', 'Page'),
                                    'source': 'webdriver'
                                })
                                new_count += 1
                    
                    print(f"  WebDriver escalation: found {new_count} additional URLs")
                finally:
                    webdriver_discovery.close()
            except Exception as e:
                print(f"  WebDriver escalation failed: {e}")
        
        # Apply locale filter
        urls = self._apply_locale_filter(urls, locale_filter)
        
        if len(urls) > 0:
            spa_count = len([u for u in urls if u.get('source') == 'spa'])
            wd_count = len([u for u in urls if u.get('source') == 'webdriver'])
            print(f"  Navigation: found {len(urls)} URLs ({spa_count} from SPA, {wd_count} from WebDriver)")
        
        return urls
    
    def _is_section_page(self, url: str) -> bool:
        """Check if URL looks like a section/index page (not a leaf content page).
        
        Conservative heuristic to avoid excessive HTTP requests in Level 1:
        - Last segment must look like a category/index name
        - URLs ending with file extensions are leaf pages
        - Deep paths (4+ segments) are always leaf pages
        """
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return False
        
        segments = path.split('/')
        last_segment = segments[-1].lower()
        
        # Deep paths are always leaf pages
        if len(segments) >= 4:
            return False
        
        # File extensions indicate leaf pages
        if '.' in last_segment:
            return False
        
        # Section/index keywords in the LAST segment indicate a section page
        section_keywords = {'guide', 'guides', 'reference', 'api', 'tutorial', 'tutorials',
                           'learn', 'docs', 'doc', 'manual', 'handbook', 'overview',
                           'getting-started', 'quickstart', 'index', 'introduction'}
        
        if last_segment in section_keywords:
            return True
        
        # Single-segment paths that are common section roots
        if len(segments) == 1 and last_segment in section_keywords:
            return True
        
        return False
    
    def _looks_like_leaf_page(self, url: str) -> bool:
        """Check if URL looks like a leaf/individual doc page."""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return False
        
        segments = path.split('/')
        last_segment = segments[-1] if segments else ''
        
        # Leaf pages often have specific file-like names or many path segments
        if len(segments) >= 4:
            return True
        
        # Specific patterns that suggest leaf pages
        leaf_indicators = ['.html', '.md', 'detail', 'specific', 'example-']
        if any(ind in last_segment.lower() for ind in leaf_indicators):
            return True
        
        return False
    
    def _extract_spa_navigation(self, html_content: str, start_url: str, base_url: str, scope_rules: ScopeRules) -> List[Dict]:
        """Extract navigation from SPA data in script tags (Next.js, Docusaurus, generic SPAs)."""
        urls = []
        seen_urls = set()
        
        # 1. Try to find __NEXT_DATA__ (Next.js)
        next_data_match = re.search(r'<script[^>]*>window\.__NEXT_DATA__\s*=\s*({.+?})</script>', html_content, re.DOTALL)
        if next_data_match:
            try:
                next_data = json.loads(next_data_match.group(1))
                if 'props' in next_data and 'pageProps' in next_data['props']:
                    page_props = next_data['props']['pageProps']
                    if 'navigation' in page_props:
                        for item in page_props['navigation']:
                            if 'url' in item or 'href' in item or 'path' in item:
                                href = item.get('url') or item.get('href') or item.get('path')
                                full_url = self._normalize_url(href, start_url, base_url)
                                if full_url and full_url not in seen_urls and scope_rules.url_matches(full_url):
                                    seen_urls.add(full_url)
                                    urls.append({
                                        'url': full_url,
                                        'title': item.get('title', item.get('label', 'Page')),
                                        'source': 'spa'
                                    })
            except (json.JSONDecodeError, KeyError):
                pass
        
        # 2. Try to find Docusaurus config
        docusaurus_match = re.search(r'<script[^>]*>window\.__DOCUSAURUS_CONFIG__\s*=\s*({.+?})</script>', html_content, re.DOTALL)
        if docusaurus_match:
            try:
                docusaurus_data = json.loads(docusaurus_match.group(1))
                if 'themeConfig' in docusaurus_data and 'navbar' in docusaurus_data['themeConfig']:
                    navbar = docusaurus_data['themeConfig']['navbar']
                    if 'items' in navbar:
                        for item in navbar['items']:
                            if 'to' in item or 'href' in item:
                                href = item.get('to') or item.get('href')
                                full_url = self._normalize_url(href, start_url, base_url)
                                if full_url and full_url not in seen_urls and scope_rules.url_matches(full_url):
                                    seen_urls.add(full_url)
                                    urls.append({
                                        'url': full_url,
                                        'title': item.get('label', 'Page'),
                                        'source': 'spa'
                                    })
            except (json.JSONDecodeError, KeyError):
                pass
        
        # 3. Generic JSON scanning in ALL script tags
        script_matches = re.findall(r'<script[^>]*>(.+?)</script>', html_content, re.DOTALL)
        for script_content in script_matches:
            if not script_content.strip():
                continue
            
            json_array_matches = re.findall(
                r'\[\s*{\s*(?:"path"|"url"|"href")\s*:\s*"([^"]+)"[^}]*?(?:"title"|"label"|"name")\s*:\s*"([^"]+)"[^}]*}[^\]]*\]',
                script_content
            )
            for path, title in json_array_matches:
                full_url = self._normalize_url(path, start_url, base_url)
                if full_url and full_url not in seen_urls and scope_rules.url_matches(full_url):
                    seen_urls.add(full_url)
                    urls.append({
                        'url': full_url,
                        'title': title[:100],
                        'source': 'spa'
                    })
        
        # 4. Extract path-like strings from JavaScript
        path_array_pattern = r'[\'"]\s*(/[a-zA-Z0-9_\-]+/[a-zA-Z0-9_\-/.]+)\s*[\'"]'
        path_matches = re.findall(path_array_pattern, html_content)
        
        _noise_prefixes = ('/static/', '/assets/', '/images/', '/img/', '/fonts/',
                           '/css/', '/js/', '/media/', '/node_modules/', '/vendor/',
                           '/_next/', '/__/', '/chunks/', '/build/', '/dist/')
        
        for path in path_matches:
            if len(path) < 4 or len(path) >= 200:
                continue
            if any(path.lower().startswith(p) for p in _noise_prefixes):
                continue
            full_url = self._normalize_url(path, start_url, base_url)
            if full_url and full_url not in seen_urls and scope_rules.url_matches(full_url):
                if self._is_doc_page(full_url):
                    seen_urls.add(full_url)
                    urls.append({
                        'url': full_url,
                        'title': path.split('/')[-1] or 'Page',
                        'source': 'spa'
                    })
        
        return urls
    
    def _extract_content_area_links(self, soup, start_url: str, base_url: str, scope_rules: ScopeRules, seen: set) -> List[Dict]:
        """Extract links from content area as fallback when nav selectors fail."""
        urls = []
        
        content_selectors = [
            'main', 'article', '.content', '#content', '.documentation',
            '.markdown', '.md-content', '[role="main"]'
        ]
        
        for selector in content_selectors:
            try:
                for content in soup.select(selector):
                    for a in content.find_all('a', href=True, limit=100):
                        href = a.get('href', '')
                        full_url = self._normalize_url(href, start_url, base_url)
                        
                        if full_url and full_url not in seen:
                            if scope_rules.url_matches(full_url) and self._is_doc_page(full_url):
                                seen.add(full_url)
                                urls.append({
                                    'url': full_url,
                                    'title': a.get_text(strip=True)[:100],
                                    'source': 'content'
                                })
                                
                                if len(urls) >= 50:
                                    return urls
            except Exception:
                continue
        
        return urls
    
    def _crawl_links(self, start_url: str, scope_rules: ScopeRules, max_pages: int, locale_filter: Optional[str] = None) -> List[Dict]:
        """Crawl links filtered by ScopeRules, following links recursively via BFS."""
        parsed_start = urlparse(start_url)
        base_url = f"{parsed_start.scheme}://{parsed_start.netloc}"
        
        visited = set()
        to_visit = deque([start_url])
        urls = []
        
        print(f"  Crawling from {start_url} (scope: {scope_rules.description[:60]})...")
        
        while to_visit and len(urls) < max_pages:
            current_url = to_visit.popleft()
            
            if current_url in visited:
                continue
            
            visited.add(current_url)
            
            # Check locale filter before adding to results
            if locale_filter:
                test_urls = [{'url': current_url, 'title': ''}]
                filtered = self._apply_locale_filter(test_urls, locale_filter)
                if not filtered:
                    continue
            
            try:
                response = self.session.get(current_url, timeout=15)
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                title = self._extract_title(soup, current_url)
                urls.append({'url': current_url, 'title': title})
                
                if len(urls) % 20 == 0:
                    print(f"    Crawled {len(urls)} pages, queue: {len(to_visit)}")
                
                for a in soup.find_all('a', href=True):
                    href = a.get('href', '')
                    full_url = self._normalize_url(href, current_url, base_url)
                    
                    if full_url and full_url not in visited and full_url not in to_visit:
                        if scope_rules.url_matches(full_url) and self._is_doc_page(full_url):
                            if locale_filter:
                                test_urls = [{'url': full_url, 'title': ''}]
                                if not self._apply_locale_filter(test_urls, locale_filter):
                                    continue
                            to_visit.append(full_url)
                
            except Exception as e:
                continue
        
        print(f"    Crawl complete: {len(urls)} pages found")
        return urls
    
    def _normalize_url(self, href: str, current_url: str, base_url: str) -> Optional[str]:
        """Normalize a URL to absolute form, stripping query params and fragments."""
        if not href or href.startswith('#') or href.startswith('javascript:'):
            return None
        
        # Strip fragment before resolving
        href = href.split('#')[0]
        if not href:
            return None
        
        if href.startswith('//'):
            full = 'https:' + href
        elif href.startswith('/'):
            full = base_url + href
        elif href.startswith('http'):
            full = href
        else:
            full = urljoin(current_url, href)
        
        # Strip query parameters to deduplicate (e.g., ?tab=js vs ?tab=python)
        parsed = urlparse(full)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    
    def _url_in_scope(self, url: str, base_url: str, scopes: List[str]) -> bool:
        """Check if URL is within any of the defined scopes."""
        if not url.startswith(base_url):
            return False
        
        # Handle both single scope (backward compat) and list of scopes
        if isinstance(scopes, str):
            scopes = [scopes]
        
        # Check if URL matches ANY scope (OR logic)
        parsed = urlparse(url)
        for scope in scopes:
            if scope == '/':
                return True
            if scope in parsed.path or parsed.path.startswith(scope.rstrip('/')):
                return True
        
        return False
    
    def _is_doc_page(self, url: str) -> bool:
        """Check if URL looks like a documentation page (extension-only filter).
        
        Path-based filtering (blog, login, etc.) is now handled by ScopeRules.
        This only filters out binary/non-HTML file extensions.
        """
        path = urlparse(url).path.lower()
        skip_extensions = {'.pdf', '.zip', '.tar', '.gz', '.png', '.jpg', '.jpeg',
                          '.gif', '.svg', '.css', '.js', '.woff', '.woff2', '.ttf',
                          '.ico', '.xml', '.json', '.mp4', '.webm', '.mp3'}
        return not any(path.endswith(ext) for ext in skip_extensions)
    
    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """Extract page title from soup."""
        # Try h1
        h1 = soup.find('h1')
        if h1:
            return h1.get_text(strip=True)[:100]
        
        # Try title tag
        title = soup.find('title')
        if title:
            return title.get_text(strip=True)[:100]
        
        # Fallback to URL
        return urlparse(url).path.split('/')[-1] or 'index'
    
    def _llm_filter_urls(self, start_url: str, urls: List[Dict]) -> List[Dict]:
        """Use LLM to filter URLs intelligently."""
        if not self.client:
            return urls
        
        # Sample URLs for analysis
        sample_size = min(100, len(urls))
        step = len(urls) // sample_size
        sample_urls = [urls[i]['url'] for i in range(0, len(urls), max(1, step))][:100]
        
        parsed = urlparse(start_url)
        
        prompt = f"""Filter documentation URLs for: {start_url}

Sample URLs ({len(sample_urls)} of {len(urls)}):
{chr(10).join(sample_urls[:50])}
...
{chr(10).join(sample_urls[-20:]) if len(sample_urls) > 50 else ''}

Create substring patterns to filter relevant documentation pages.

Rules:
1. Include patterns should match the target documentation path
2. Exclude patterns should filter out: other languages, old versions, blog, downloads
3. Use simple substrings like "/en/5." or "/docs/"

Return JSON:
{{"include_patterns": ["/path/"], "exclude_patterns": ["/blog/"], "reasoning": "brief"}}"""

        try:
            response = self.client.chat.completions.create(
                model="anthropic/claude-haiku-4.5",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            result = response.choices[0].message.content.strip()
            if result.startswith("```"):
                result = re.sub(r'^```\w*\n?', '', result)
                result = re.sub(r'\n?```$', '', result)
            
            filters = json.loads(result)
            print(f"  LLM Filter: {filters.get('reasoning', '')[:60]}")
            
            # Apply filters
            include = filters.get('include_patterns', [])
            exclude = filters.get('exclude_patterns', [])
            
            filtered = []
            for u in urls:
                url = u['url']
                
                # Check excludes
                if any(p in url for p in exclude):
                    continue
                
                # Check includes
                if include and not any(p in url for p in include):
                    continue
                
                filtered.append(u)
            
            print(f"  Filtered: {len(urls)} -> {len(filtered)}")
            return filtered if filtered else urls
            
        except Exception as e:
            print(f"  LLM filter failed: {e}")
            return urls


def discover_documentation_urls(start_url: str, max_pages: int = 500) -> Dict:
    """Convenience function to discover documentation URLs."""
    discovery = URLDiscovery()
    return discovery.discover_urls(start_url, max_pages)
